{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing libraries\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "import json\n",
    "import pickle\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout\n",
    "from keras.optimizers import SGD\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "830 documents\n",
      "210 classes ['Anger Management', 'Anxiety', 'Cognitive Restructuring', 'Coping Strategies', 'Coping with Stress', 'Coping with Trauma', 'Harm Reduction', 'Identifying a Drug User', 'Legal Issues', 'Meditation Practices', 'Need for Change', 'Nutrition and Wellness', 'Overcoming Shame', 'Peer Support', 'Personal Boundaries', 'Recovery from Drug Abuse', 'Relapse Prevention', 'Resilience Building', 'Self-Care Practices', 'Social Anxiety', 'Substance Education', 'Support Systems', 'What are the types of depression?', 'Withdrawal Symptoms', 'Withdrawal Symptoms Strategies', 'about', 'academic_goal_setting', 'academic_performance', 'addiction', 'afternoon', 'anti_racism_resources', 'anxious', 'ask', 'at what age does anxiety peak?', 'body_dysmorphia_support', 'building_resilience', 'burden', 'can lack of sleep make you feel sad?', 'can low blood sugar cause suicidal thoughts?', 'casual', 'connecting_with_peers', 'coping_strategies', 'creation', 'creative_expression', 'daily_routines', 'death', 'default', 'depressed', 'depression_intro', 'do we control our thoughts?', 'does oversleeping cause depression?', 'done', 'drug use Patterns', 'encouragement', 'encouraging_activity', 'encouraging_self_help', 'evening', 'exploring_hobbies', 'fact-1', 'fact-10', 'fact-11', 'fact-12', 'fact-13', 'fact-14', 'fact-15', 'fact-16', 'fact-17', 'fact-18', 'fact-19', 'fact-2', 'fact-20', 'fact-21', 'fact-22', 'fact-23', 'fact-24', 'fact-25', 'fact-26', 'fact-27', 'fact-28', 'fact-29', 'fact-3', 'fact-30', 'fact-31', 'fact-32', 'fact-5', 'fact-6', 'fact-7', 'fact-8', 'fact-9', 'family_support', 'finding_sports_passion', 'friends', 'goodbye', 'greeting', 'happy', 'hate-me', 'hate-you', 'help', 'how can we reduce anxiety?', 'how does depression affect the world?', 'how long can anxiety last?', 'how many thoughts a day do we have?', 'i am a victim of bullying', 'i am afraid i will fail again', 'i am afraid to file a case against bullying', 'i am feeling anxious lately.', 'i am feeling stressed lately', 'i am good for nothing!', 'i am good for nothing.', 'i am lonely!', 'i am sad', 'i am stressed out', \"i can't do this anymore\", 'i feel i have let my parents down', 'i hate losing.', 'i hate myself!', 'i let everyojokne down', 'i think i am ugly!', \"i think i'm losing my mind\", 'i want a break', 'i want to kill myself', 'i want to leave the cou ntry and run away', 'i will never succeed in life', \"i wish i could've been a winner\", 'i wish i was better than them', 'i wish to quit', 'increment', 'is depression a side effect of diabetes?', 'is school a cause of depression?', 'jokes', 'learn-mental-health', 'learn-more', 'location', 'loneliness', 'media_influence_body_image', 'medication', 'meditation', 'mental-health-fact', 'mindfulness', 'morning', 'motivation', 'my time has come', 'neutral-response', 'night', 'no one likes me!', 'no-approach', 'no-response', 'no_girlfriend_boyfriend_loneliness', 'not-talking', 'nutrition_and_depression', 'overcoming_setbacks', 'pandora-useful', 'positive_affirmation', 'positive_body_image', 'positive_outlook', 'problem', 'promotion_denied', 'promotion_nervousness', 'promotion_seek', 'racism_experiences', 'recognizing_progress', 'relationship_expectations', 'relationships', 'repeat', 'reporting_racism', 'sad', 'scared', 'seeking_help', 'seeking_professional_help', 'self_assessment', 'self_care', 'self_harm', 'self_love_before_relationship', 'skill', 'sleep', 'sleep_and_depression', 'social_connection', 'something-else', 'sports_injury_recovery', 'sports_motivation', 'stress_due_to_marks', 'stress_management', 'stressed', 'struggle drug abuse', 'stupid', 'suicide', 'support_system', 'thanks', 'therapy', 'tracking_mood', 'understand', 'unplaced', 'user-advice', 'user-agree', 'user-meditation', 'what are the causes of depression?', 'what are the stages of anxiety?', 'what are the top causes of depression?', 'what is depression?', 'what is the 3 3 3 rule for anxiety?', 'what is the biological cause of depression?', 'what is the meaning of anxiety and depression?', 'which age group has the highest rate of depression?', 'which country has the highest rate of depression?', 'which country has the lowest rate of depression?', 'which race has the highest rate of depression?', 'why is anxiety bad for you?', 'work_and_depression', 'worthless', 'wrong']\n",
      "883 unique lemmatized words [\"'ll\", \"'m\", \"'re\", \"'s\", \"'ve\", ',', '.', '3', 'a', 'about', 'absolutely', 'abuse', 'academic', 'achieving', 'acknowledging', 'action', 'activity', 'addiction', 'address', 'advancement', 'advancing', 'advice', 'advisor', 'affect', 'affecting', 'afraid', 'after', 'afternoon', 'again', 'against', 'age', 'aid', 'alcohol', 'all', 'alleviate', 'alleviating', 'alone', 'alot', 'already', 'also', 'am', 'an', 'and', 'anger', 'another', 'answer', 'anti-racism', 'antidepressant', 'anuone', 'anxiety', 'anxious', 'any', 'anymore', 'anyone', 'anything', 'appearance', 'appears', 'approach', 'approaching', 'are', 'art', 'article', 'ashamed', 'ask', 'ass', 'assist', 'assistance', 'associated', 'at', 'attack', 'au', 'available', 'avoid', 'avoiding', 'away', 'awful', 'back', 'bad', 'balance', 'balancing', 'basis', 'battling', 'be', 'bearable', 'beauty', 'because', 'become', 'becomes', 'becoming', 'been', 'before', 'begin', 'behavior', 'behavioral', 'behind', 'benefit', 'best', 'better', 'between', 'bias', 'biological', 'blood', 'body', 'bonjour', 'book', 'both', 'bounce', 'boundary', 'boyfriend', 'break', 'breaking', 'breathe', 'bring', 'brother', 'build', 'building', 'bullying', 'burden', 'burned', 'but', 'by', 'bye', 'ca', 'call', 'calming', 'can', 'care', 'career', 'case', 'cause', 'caused', 'causing', 'celebrate', 'celebrating', 'challenge', 'challenging', 'change', 'chapter', 'charge', 'chat', 'check', 'cheerful', 'child', 'classmate', 'close', 'cognitive', 'college', 'come', 'commit', 'commitment', 'committed', 'common', 'communicate', 'community', 'companionship', 'concern', 'concerned', 'confidence', 'connect', 'connection', 'consequence', 'consider', 'considering', 'consult', 'continue', 'contribute', 'control', 'cope', 'coping', 'cou', 'could', 'counseling', 'counselor', 'country', 'craving', 'crazy', 'create', 'created', 'creating', 'creative', 'creativity', 'crisis', 'crucial', 'cue', 'cultivate', 'cultivating', 'cure', 'curious', 'cycle', 'dad', 'daily', 'dangerous', 'dating', 'day', 'deal', 'dealing', 'decided', 'define', 'denied', 'dependency', 'depressed', 'depression', 'depressive', 'deserve', 'determined', 'diabetes', 'did', 'die', 'died', 'dietary', 'difference', 'different', 'differentiate', 'difficult', 'disappointment', 'discomfort', 'disconnected', 'discouraged', 'discovering', 'discrimination', 'disorder', 'distorted', 'distract', 'disturbance', 'do', 'doe', 'doing', 'down', 'drinking', 'drug', 'drug-free', 'due', 'dumb', 'during', 'dysmorphia', 'ease', 'easier', 'eating', 'educate', 'educating', 'educational', 'effect', 'effective', 'effectively', 'else', 'embracing', 'emotional', 'emotionally', 'employ', 'employment', 'empowering', 'empty', 'encountering', 'encourage', 'end', 'engage', 'enjoy', 'enough', 'environment', 'episode', 'establish', 'establishing', 'evening', 'everting', 'everyday', 'everyojokne', 'everyone', 'everything', 'exam', 'exercise', 'exist', 'expect', 'expectation', 'experience', 'experienced', 'experiencing', 'explore', 'exploring', 'expression', 'face', 'facing', 'fact', 'fail', 'failure', 'family', 'fare', 'fear', 'feel', 'feeling', 'few', 'file', 'finance', 'financial', 'find', 'finding', 'fine', 'first', 'focus', 'for', 'forgeting', 'form', 'forward', 'fragile', 'free', 'fresh', 'friend', 'from', 'future', 'general', 'get', 'getting', 'girlfriend', 'girlfriend/boyfriend', 'give', 'go', 'goal', 'going', 'good', 'goodbye', 'grade', 'great', 'group', 'guess', 'guidance', 'guide', 'guilt', 'ha', 'habit', 'hack', 'had', 'hand', 'handle', 'handling', 'happy', 'hard', 'harm', 'hate', 'haulting', 'have', 'having', 'hay', 'healing', 'health', 'healthier', 'healthy', 'hello', 'help', 'helpful', 'helpline', 'here', 'hey', 'hi', 'highest', 'hinders', 'hmmm', 'hobby', 'hola', 'holistic', 'home', 'hope', 'hopeless', 'how', 'howdy', 'i', 'identify', 'if', 'ill', 'illness', 'image', 'immediate', 'impact', 'impending', 'importance', 'important', 'improve', 'improvement', 'improving', 'in', 'incident', 'include', 'incorporating', 'increment', 'indicate', 'indicating', 'indicator', 'individual', 'influence', 'information', 'initiate', 'injury', 'insight', 'insominia', 'insomnia', 'intensity', 'interest', 'interested', 'interpersonal', 'into', 'involve', 'involved', 'involving', 'is', 'isolated', 'isolation', 'issue', 'it', 'job', 'joke', 'journal', 'journey', 'joy', 'just', 'k', 'kill', 'killing', 'know', 'lack', 'last', 'lately', 'later', 'learn', 'learning', 'leave', 'left', 'legal', 'let', 'life', 'lifestyle', 'like', 'live', 'living', 'location', 'loneliness', 'lonely', 'long', 'long-term', 'look', 'looking', 'losing', 'lost', 'lot', 'love', 'loved', 'low', 'lowest', 'made', 'maintain', 'maintaining', 'make', 'manage', 'manageable', 'management', 'managing', 'many', 'mark', 'may', 'me', 'mean', 'meaning', 'meaningful', 'measuring', 'mechanism', 'media-driven', 'medication', 'meditation', 'medium', 'meeting', 'member', 'mental', 'mentally', 'mentioned', 'mentor', 'midst', 'might', 'milestone', 'mind', 'mindfulness', 'minimize', 'misuse', 'mitigate', 'mom', 'moment', 'money', 'monitoring', 'mood', 'more', 'morning', 'most', 'motivated', 'motivation', 'moving', 'much', 'my', 'myself', \"n't\", 'name', 'natural', 'navigate', 'need', 'needed', 'negatively', 'nervous', 'nervousness', 'network', 'never', 'new', 'next', 'nice', 'night', 'no', 'nobody', 'normal', 'not', 'nothing', 'now', 'ntry', 'nutrition', 'nutritional', 'obstacle', 'of', 'offer', 'often', 'oh', 'ok', 'okay', 'on', 'once', 'one', 'online', 'only', 'open', 'optimism', 'option', 'or', 'other', 'others', 'our', 'out', 'outlook', 'overcome', 'overcoming', 'oversleeping', 'overwhelmed', 'overwhelming', 'own', 'pain', 'panic', 'parent', 'passed', 'passion', 'past', 'path', 'pattern', 'peak', 'peer', 'people', 'performance', 'person', 'personal', 'personalized', 'physical', 'placed', 'plan', 'platform', 'play', 'please', 'portrayal', 'positive', 'possibly', 'practical', 'practice', 'practicing', 'prepared', 'preparing', 'pressure', 'prevent', 'preventing', 'prevention', 'prioritize', 'probably', 'problem', 'proceed', 'process', 'professional', 'program', 'progress', 'prolonged', 'promoted', 'promotion', 'proper', 'protect', 'provide', 'provides', 'public', 'purpose', 'quit', 'quitting', 'race', 'racial', 'racism', 'racist', 'rate', 'reach', 'reaching', 'ready', 'realistic', 'really', 'rebuilding', 'recognize', 'recognizing', 'recommend', 'recommendation', 'recover', 'recovering', 'recovery', 'reduce', 'reducing', 'reduction', 'regain', 'regarding', 'rejection', 'rekindling', 'relapse', 'relapsing', 'related', 'relationship', 'relax', 'relaxation', 'reliance', 'relief', 'remedy', 'remember', 'repeating', 'repercussion', 'report', 'reporting', 'requires', 'resilience', 'resilience-building', 'resistance', 'resource', 'response', 'responsibility', 'restructure', 'restructuring', 'returning', 'revoir', 'right', 'robot', 'role', 'routine', 'rule', 'run', 'sad', 'sadness', 'safe', 'said', 'sasa', 'say', 'saying', 'sayonara', 'scared', 'schedule', 'school', 'search', 'see', 'seek', 'seeking', 'seem', 'seems', 'self-assessment', 'self-care', 'self-destructive', 'self-esteem', 'self-harm', 'self-harming', 'self-help', 'self-love', 'self-worth', 'senior', 'sense', 'serious', 'service', 'set', 'setback', 'setting', 'severity', 'shame', 'share', 'shared', 'sharing', 'should', 'shut', 'shyness', 'side', 'sign', 'significant', 'singlehood', 'sister', 'situation', 'sleep', 'slept', 'small', 'smaller', 'smoking', 'smoother', 'so', 'sober', 'sobriety', 'social', 'societal', 'solidarity', 'some', 'someone', 'something', 'sound', 'specific', 'sport', 'spot', 'stage', 'standard', 'start', 'started', 'starting', 'state', 'stay', 'step', 'still', 'story', 'strategy', 'strength', 'strengthen', 'stress', 'stressed', 'stressful', 'structure', 'struggle', 'struggling', 'stuck', 'student', 'study', 'stupid', 'subside', 'substance', 'succeed', 'success', 'suffering', 'sugar', 'suggest', 'suggestion', 'suicidal', 'suicide', 'support', 'supporting', 'supportive', 'sure', 'suspect', 'symptom', 'system', 'tailored', 'take', 'taking', 'talk', 'talking', 'task', 'teacher', 'team', 'technique', 'tell', 'tempted', 'than', 'thank', 'thanks', 'that', 'the', 'thee', 'their', 'them', 'then', 'therapist', 'therapy', 'there', 'these', 'thing', 'think', 'thinking', 'this', 'thought', 'through', 'time', 'timeline', 'tip', 'tired', 'to', 'today', 'told', 'too', 'tool', 'top', 'tough', 'toughness', 'track', 'tracking', 'transform', 'transition', 'trauma', 'treating', 'treatment', 'trigger', 'trust', 'trusted', 'trying', 'turn', 'turning', 'type', 'typical', 'ugly', 'understand', 'understanding', 'understands', 'unemployed', 'unmotivated', 'unplaced', 'unrealistic', 'unsupported', 'unwell', 'up', 'urge', 'use', 'used', 'useful', 'useless', 'using', 'usually', 'very', 'victim', 'victory', 'visit', 'wa', 'want', 'warning', 'watch', 'way', 'we', 'website', 'weight', 'well', 'well-being', 'wellness', 'were', 'what', 'whatever', 'when', 'where', 'which', 'while', 'who', 'why', 'will', 'winner', 'wish', 'with', 'withdrawal', 'without', 'work', 'work-life', 'work-related', 'workload', 'workout', 'workplace', 'world', 'worried', 'worry', 'worsening', 'worthless', 'would', 'wrong', 'yeah', 'yes', 'yet', 'you', 'your', 'yourself']\n"
     ]
    }
   ],
   "source": [
    "# tokenizing and lematizing\n",
    "words=[]\n",
    "classes = []\n",
    "documents = []\n",
    "ignore_words = ['?', '!']\n",
    "data_file = open('intents.json').read()\n",
    "intents = json.loads(data_file)\n",
    "\n",
    "for intent in intents['intents']:\n",
    "    for pattern in intent['patterns']:\n",
    "        #tokenize each word\n",
    "        w = nltk.word_tokenize(pattern)\n",
    "        words.extend(w)\n",
    "        #add documents in the corpus\n",
    "        documents.append((w, intent['tag']))\n",
    "        # add to our classes list\n",
    "        if intent['tag'] not in classes:\n",
    "            classes.append(intent['tag'])\n",
    "# lemmatize and lower each word and remove duplicates\n",
    "words = [lemmatizer.lemmatize(w.lower()) for w in words if w not in ignore_words]\n",
    "words = sorted(list(set(words)))\n",
    "# sort classes\n",
    "classes = sorted(list(set(classes)))\n",
    "# documents = combination between patterns and intents\n",
    "print (len(documents), \"documents\")\n",
    "# classes = intents\n",
    "print (len(classes), \"classes\", classes)\n",
    "# words = all words, vocabulary\n",
    "print (len(words), \"unique lemmatized words\", words)\n",
    "pickle.dump(words,open('texts.pkl','wb'))\n",
    "pickle.dump(classes,open('labels.pkl','wb'))\n",
    "# create our training data\n",
    "training = []\n",
    "# create an empty array for our output\n",
    "output_empty = [0] * len(classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# training set, bag of words for each sentence\n",
    "for doc in documents:\n",
    "    # initialize our bag of words\n",
    "    bag = []\n",
    "    # list of tokenized words for the pattern\n",
    "    pattern_words = doc[0]\n",
    "    # lemmatize each word - create base word, in attempt to represent related words\n",
    "    pattern_words = [lemmatizer.lemmatize(word.lower()) for word in pattern_words]\n",
    "    # create our bag of words array with 1, if word match found in current pattern\n",
    "    for w in words:\n",
    "        bag.append(1) if w in pattern_words else bag.append(0)\n",
    "    \n",
    "    # output is a '0' for each tag and '1' for current tag (for each pattern)\n",
    "    output_row = list(output_empty)\n",
    "    output_row[classes.index(doc[1])] = 1\n",
    "    \n",
    "    training.append([bag, output_row])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "830\n"
     ]
    }
   ],
   "source": [
    "# shuffle our features and turn into np.array\n",
    "print(len(training))\n",
    "random.shuffle(training)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data created\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# training = np.array(training)\n",
    "# create train and test lists. X - patterns, Y - intents\n",
    "train_x = []\n",
    "train_y = []\n",
    "for i in training:\n",
    "    train_x.append(i[0])\n",
    "    train_y.append(i[1])\n",
    "# train_x = list(training[:,0])\n",
    "# train_y = list(training[:,1])\n",
    "print(\"Training data created\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model - 3 layers. First layer 128 neurons, second layer 64 neurons and 3rd output layer contains number of neurons\n",
    "# equal to number of intents to predict output intent with softmax\n",
    "model = Sequential()\n",
    "model.add(Dense(128, input_shape=(len(train_x[0]),), activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(len(train_y[0]), activation='softmax'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.SGD.\n"
     ]
    }
   ],
   "source": [
    "# Compile model. Stochastic gradient descent with Nesterov accelerated gradient gives good results for this model\n",
    "sgd = SGD(lr=0.01,  momentum=0.9, nesterov=True)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy', 'categorical_accuracy', 'top_k_categorical_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 5.1959 - accuracy: 0.0373 - categorical_accuracy: 0.0373 - top_k_categorical_accuracy: 0.1217\n",
      "Epoch 2/200\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 4.7596 - accuracy: 0.0651 - categorical_accuracy: 0.0651 - top_k_categorical_accuracy: 0.1807\n",
      "Epoch 3/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 4.4232 - accuracy: 0.1133 - categorical_accuracy: 0.1133 - top_k_categorical_accuracy: 0.2494\n",
      "Epoch 4/200\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 4.2458 - accuracy: 0.1614 - categorical_accuracy: 0.1614 - top_k_categorical_accuracy: 0.3108\n",
      "Epoch 5/200\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 4.0496 - accuracy: 0.1880 - categorical_accuracy: 0.1880 - top_k_categorical_accuracy: 0.3386\n",
      "Epoch 6/200\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 3.9401 - accuracy: 0.2060 - categorical_accuracy: 0.2060 - top_k_categorical_accuracy: 0.3807\n",
      "Epoch 7/200\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 3.7682 - accuracy: 0.2241 - categorical_accuracy: 0.2241 - top_k_categorical_accuracy: 0.4157\n",
      "Epoch 8/200\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 3.6747 - accuracy: 0.2398 - categorical_accuracy: 0.2398 - top_k_categorical_accuracy: 0.4325\n",
      "Epoch 9/200\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 3.5521 - accuracy: 0.2578 - categorical_accuracy: 0.2578 - top_k_categorical_accuracy: 0.4542\n",
      "Epoch 10/200\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 3.3915 - accuracy: 0.2795 - categorical_accuracy: 0.2795 - top_k_categorical_accuracy: 0.4783\n",
      "Epoch 11/200\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 3.2558 - accuracy: 0.2928 - categorical_accuracy: 0.2928 - top_k_categorical_accuracy: 0.5060\n",
      "Epoch 12/200\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 3.2343 - accuracy: 0.2964 - categorical_accuracy: 0.2964 - top_k_categorical_accuracy: 0.5024\n",
      "Epoch 13/200\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 3.0833 - accuracy: 0.3145 - categorical_accuracy: 0.3145 - top_k_categorical_accuracy: 0.5313\n",
      "Epoch 14/200\n",
      "166/166 [==============================] - 1s 4ms/step - loss: 3.0098 - accuracy: 0.3133 - categorical_accuracy: 0.3133 - top_k_categorical_accuracy: 0.5518\n",
      "Epoch 15/200\n",
      "166/166 [==============================] - 1s 4ms/step - loss: 2.9169 - accuracy: 0.3422 - categorical_accuracy: 0.3422 - top_k_categorical_accuracy: 0.5663\n",
      "Epoch 16/200\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 2.8634 - accuracy: 0.3301 - categorical_accuracy: 0.3301 - top_k_categorical_accuracy: 0.5940\n",
      "Epoch 17/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 2.6688 - accuracy: 0.3807 - categorical_accuracy: 0.3807 - top_k_categorical_accuracy: 0.6313\n",
      "Epoch 18/200\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 2.6098 - accuracy: 0.3807 - categorical_accuracy: 0.3807 - top_k_categorical_accuracy: 0.6133\n",
      "Epoch 19/200\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 2.6471 - accuracy: 0.3723 - categorical_accuracy: 0.3723 - top_k_categorical_accuracy: 0.6337\n",
      "Epoch 20/200\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 2.5177 - accuracy: 0.3988 - categorical_accuracy: 0.3988 - top_k_categorical_accuracy: 0.6494\n",
      "Epoch 21/200\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 2.4064 - accuracy: 0.4108 - categorical_accuracy: 0.4108 - top_k_categorical_accuracy: 0.6771\n",
      "Epoch 22/200\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 2.3729 - accuracy: 0.4193 - categorical_accuracy: 0.4193 - top_k_categorical_accuracy: 0.6735\n",
      "Epoch 23/200\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 2.3208 - accuracy: 0.4181 - categorical_accuracy: 0.4181 - top_k_categorical_accuracy: 0.6940\n",
      "Epoch 24/200\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 2.3096 - accuracy: 0.4313 - categorical_accuracy: 0.4313 - top_k_categorical_accuracy: 0.7145\n",
      "Epoch 25/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 2.2139 - accuracy: 0.4386 - categorical_accuracy: 0.4386 - top_k_categorical_accuracy: 0.7253\n",
      "Epoch 26/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 2.1660 - accuracy: 0.4494 - categorical_accuracy: 0.4494 - top_k_categorical_accuracy: 0.7398\n",
      "Epoch 27/200\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 2.0625 - accuracy: 0.4976 - categorical_accuracy: 0.4976 - top_k_categorical_accuracy: 0.7301\n",
      "Epoch 28/200\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 2.0787 - accuracy: 0.4747 - categorical_accuracy: 0.4747 - top_k_categorical_accuracy: 0.7398\n",
      "Epoch 29/200\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 2.0390 - accuracy: 0.4675 - categorical_accuracy: 0.4675 - top_k_categorical_accuracy: 0.7398\n",
      "Epoch 30/200\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 2.0454 - accuracy: 0.4747 - categorical_accuracy: 0.4747 - top_k_categorical_accuracy: 0.7578\n",
      "Epoch 31/200\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1.9510 - accuracy: 0.5036 - categorical_accuracy: 0.5036 - top_k_categorical_accuracy: 0.7843\n",
      "Epoch 32/200\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 2.0139 - accuracy: 0.4964 - categorical_accuracy: 0.4964 - top_k_categorical_accuracy: 0.7506\n",
      "Epoch 33/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1.9209 - accuracy: 0.5000 - categorical_accuracy: 0.5000 - top_k_categorical_accuracy: 0.7735\n",
      "Epoch 34/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1.8721 - accuracy: 0.5193 - categorical_accuracy: 0.5193 - top_k_categorical_accuracy: 0.7747\n",
      "Epoch 35/200\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1.8908 - accuracy: 0.4964 - categorical_accuracy: 0.4964 - top_k_categorical_accuracy: 0.7807\n",
      "Epoch 36/200\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1.8311 - accuracy: 0.5373 - categorical_accuracy: 0.5373 - top_k_categorical_accuracy: 0.7952\n",
      "Epoch 37/200\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1.7561 - accuracy: 0.5325 - categorical_accuracy: 0.5325 - top_k_categorical_accuracy: 0.7976\n",
      "Epoch 38/200\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1.8355 - accuracy: 0.5229 - categorical_accuracy: 0.5229 - top_k_categorical_accuracy: 0.7867\n",
      "Epoch 39/200\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1.7728 - accuracy: 0.5313 - categorical_accuracy: 0.5313 - top_k_categorical_accuracy: 0.8084\n",
      "Epoch 40/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1.7662 - accuracy: 0.5398 - categorical_accuracy: 0.5398 - top_k_categorical_accuracy: 0.8157\n",
      "Epoch 41/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1.7828 - accuracy: 0.5229 - categorical_accuracy: 0.5229 - top_k_categorical_accuracy: 0.7940\n",
      "Epoch 42/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1.6884 - accuracy: 0.5530 - categorical_accuracy: 0.5530 - top_k_categorical_accuracy: 0.8205\n",
      "Epoch 43/200\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1.6428 - accuracy: 0.5651 - categorical_accuracy: 0.5651 - top_k_categorical_accuracy: 0.8217\n",
      "Epoch 44/200\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1.6810 - accuracy: 0.5482 - categorical_accuracy: 0.5482 - top_k_categorical_accuracy: 0.8325\n",
      "Epoch 45/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1.6295 - accuracy: 0.5795 - categorical_accuracy: 0.5795 - top_k_categorical_accuracy: 0.8217\n",
      "Epoch 46/200\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1.6775 - accuracy: 0.5410 - categorical_accuracy: 0.5410 - top_k_categorical_accuracy: 0.8253\n",
      "Epoch 47/200\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1.6707 - accuracy: 0.5590 - categorical_accuracy: 0.5590 - top_k_categorical_accuracy: 0.8229\n",
      "Epoch 48/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1.4962 - accuracy: 0.6000 - categorical_accuracy: 0.6000 - top_k_categorical_accuracy: 0.8386\n",
      "Epoch 49/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1.5631 - accuracy: 0.5687 - categorical_accuracy: 0.5687 - top_k_categorical_accuracy: 0.8614\n",
      "Epoch 50/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1.5632 - accuracy: 0.5880 - categorical_accuracy: 0.5880 - top_k_categorical_accuracy: 0.8434\n",
      "Epoch 51/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1.5084 - accuracy: 0.5928 - categorical_accuracy: 0.5928 - top_k_categorical_accuracy: 0.8373\n",
      "Epoch 52/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1.5060 - accuracy: 0.6060 - categorical_accuracy: 0.6060 - top_k_categorical_accuracy: 0.8470\n",
      "Epoch 53/200\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1.4962 - accuracy: 0.5928 - categorical_accuracy: 0.5928 - top_k_categorical_accuracy: 0.8458\n",
      "Epoch 54/200\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1.5085 - accuracy: 0.6000 - categorical_accuracy: 0.6000 - top_k_categorical_accuracy: 0.8361\n",
      "Epoch 55/200\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1.5521 - accuracy: 0.6012 - categorical_accuracy: 0.6012 - top_k_categorical_accuracy: 0.8301\n",
      "Epoch 56/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1.5268 - accuracy: 0.6084 - categorical_accuracy: 0.6084 - top_k_categorical_accuracy: 0.8506\n",
      "Epoch 57/200\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1.4865 - accuracy: 0.5964 - categorical_accuracy: 0.5964 - top_k_categorical_accuracy: 0.8554\n",
      "Epoch 58/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1.4978 - accuracy: 0.6012 - categorical_accuracy: 0.6012 - top_k_categorical_accuracy: 0.8373\n",
      "Epoch 59/200\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 1.3666 - accuracy: 0.6289 - categorical_accuracy: 0.6289 - top_k_categorical_accuracy: 0.8639\n",
      "Epoch 60/200\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 1.4155 - accuracy: 0.6277 - categorical_accuracy: 0.6277 - top_k_categorical_accuracy: 0.8494\n",
      "Epoch 61/200\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1.5101 - accuracy: 0.5783 - categorical_accuracy: 0.5783 - top_k_categorical_accuracy: 0.8542\n",
      "Epoch 62/200\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1.5026 - accuracy: 0.6072 - categorical_accuracy: 0.6072 - top_k_categorical_accuracy: 0.8614\n",
      "Epoch 63/200\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1.4435 - accuracy: 0.6000 - categorical_accuracy: 0.6000 - top_k_categorical_accuracy: 0.8530\n",
      "Epoch 64/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1.3424 - accuracy: 0.6349 - categorical_accuracy: 0.6349 - top_k_categorical_accuracy: 0.8723\n",
      "Epoch 65/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1.4732 - accuracy: 0.5904 - categorical_accuracy: 0.5904 - top_k_categorical_accuracy: 0.8614\n",
      "Epoch 66/200\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 1.4044 - accuracy: 0.6398 - categorical_accuracy: 0.6398 - top_k_categorical_accuracy: 0.8590\n",
      "Epoch 67/200\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1.4656 - accuracy: 0.6325 - categorical_accuracy: 0.6325 - top_k_categorical_accuracy: 0.8578\n",
      "Epoch 68/200\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1.5613 - accuracy: 0.5940 - categorical_accuracy: 0.5940 - top_k_categorical_accuracy: 0.8554\n",
      "Epoch 69/200\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1.4925 - accuracy: 0.6108 - categorical_accuracy: 0.6108 - top_k_categorical_accuracy: 0.8578\n",
      "Epoch 70/200\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1.4017 - accuracy: 0.6012 - categorical_accuracy: 0.6012 - top_k_categorical_accuracy: 0.8675\n",
      "Epoch 71/200\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1.3763 - accuracy: 0.6181 - categorical_accuracy: 0.6181 - top_k_categorical_accuracy: 0.8663\n",
      "Epoch 72/200\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1.4035 - accuracy: 0.6301 - categorical_accuracy: 0.6301 - top_k_categorical_accuracy: 0.8627\n",
      "Epoch 73/200\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1.4309 - accuracy: 0.6157 - categorical_accuracy: 0.6157 - top_k_categorical_accuracy: 0.8675\n",
      "Epoch 74/200\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1.3536 - accuracy: 0.6325 - categorical_accuracy: 0.6325 - top_k_categorical_accuracy: 0.8675\n",
      "Epoch 75/200\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1.3080 - accuracy: 0.6530 - categorical_accuracy: 0.6530 - top_k_categorical_accuracy: 0.8855\n",
      "Epoch 76/200\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1.3933 - accuracy: 0.6554 - categorical_accuracy: 0.6554 - top_k_categorical_accuracy: 0.8867\n",
      "Epoch 77/200\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1.4160 - accuracy: 0.6277 - categorical_accuracy: 0.6277 - top_k_categorical_accuracy: 0.8578\n",
      "Epoch 78/200\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1.3274 - accuracy: 0.6482 - categorical_accuracy: 0.6482 - top_k_categorical_accuracy: 0.8699\n",
      "Epoch 79/200\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1.4072 - accuracy: 0.6337 - categorical_accuracy: 0.6337 - top_k_categorical_accuracy: 0.8783\n",
      "Epoch 80/200\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1.4158 - accuracy: 0.6386 - categorical_accuracy: 0.6386 - top_k_categorical_accuracy: 0.8675\n",
      "Epoch 81/200\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1.4524 - accuracy: 0.6157 - categorical_accuracy: 0.6157 - top_k_categorical_accuracy: 0.8566\n",
      "Epoch 82/200\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1.2861 - accuracy: 0.6410 - categorical_accuracy: 0.6410 - top_k_categorical_accuracy: 0.8807\n",
      "Epoch 83/200\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1.4787 - accuracy: 0.6181 - categorical_accuracy: 0.6181 - top_k_categorical_accuracy: 0.8542\n",
      "Epoch 84/200\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1.3595 - accuracy: 0.6470 - categorical_accuracy: 0.6470 - top_k_categorical_accuracy: 0.8663\n",
      "Epoch 85/200\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1.3795 - accuracy: 0.6482 - categorical_accuracy: 0.6482 - top_k_categorical_accuracy: 0.8602\n",
      "Epoch 86/200\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1.4013 - accuracy: 0.6446 - categorical_accuracy: 0.6446 - top_k_categorical_accuracy: 0.8759\n",
      "Epoch 87/200\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1.2811 - accuracy: 0.6542 - categorical_accuracy: 0.6542 - top_k_categorical_accuracy: 0.8952\n",
      "Epoch 88/200\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1.3418 - accuracy: 0.6434 - categorical_accuracy: 0.6434 - top_k_categorical_accuracy: 0.8771\n",
      "Epoch 89/200\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1.3354 - accuracy: 0.6325 - categorical_accuracy: 0.6325 - top_k_categorical_accuracy: 0.8795\n",
      "Epoch 90/200\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1.3538 - accuracy: 0.6349 - categorical_accuracy: 0.6349 - top_k_categorical_accuracy: 0.8855\n",
      "Epoch 91/200\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1.3090 - accuracy: 0.6386 - categorical_accuracy: 0.6386 - top_k_categorical_accuracy: 0.8807\n",
      "Epoch 92/200\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1.3174 - accuracy: 0.6518 - categorical_accuracy: 0.6518 - top_k_categorical_accuracy: 0.8880\n",
      "Epoch 93/200\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1.2743 - accuracy: 0.6711 - categorical_accuracy: 0.6711 - top_k_categorical_accuracy: 0.8807\n",
      "Epoch 94/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1.3529 - accuracy: 0.6518 - categorical_accuracy: 0.6518 - top_k_categorical_accuracy: 0.8711\n",
      "Epoch 95/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1.3542 - accuracy: 0.6337 - categorical_accuracy: 0.6337 - top_k_categorical_accuracy: 0.8651\n",
      "Epoch 96/200\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1.2914 - accuracy: 0.6458 - categorical_accuracy: 0.6458 - top_k_categorical_accuracy: 0.8855\n",
      "Epoch 97/200\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1.2957 - accuracy: 0.6410 - categorical_accuracy: 0.6410 - top_k_categorical_accuracy: 0.8819\n",
      "Epoch 98/200\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1.2412 - accuracy: 0.6675 - categorical_accuracy: 0.6675 - top_k_categorical_accuracy: 0.8904\n",
      "Epoch 99/200\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1.4405 - accuracy: 0.6386 - categorical_accuracy: 0.6386 - top_k_categorical_accuracy: 0.8687\n",
      "Epoch 100/200\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1.4895 - accuracy: 0.6349 - categorical_accuracy: 0.6349 - top_k_categorical_accuracy: 0.8807\n",
      "Epoch 101/200\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1.4016 - accuracy: 0.6361 - categorical_accuracy: 0.6361 - top_k_categorical_accuracy: 0.8675\n",
      "Epoch 102/200\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1.4313 - accuracy: 0.6301 - categorical_accuracy: 0.6301 - top_k_categorical_accuracy: 0.8735\n",
      "Epoch 103/200\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1.3610 - accuracy: 0.6410 - categorical_accuracy: 0.6410 - top_k_categorical_accuracy: 0.8855\n",
      "Epoch 104/200\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1.2710 - accuracy: 0.6639 - categorical_accuracy: 0.6639 - top_k_categorical_accuracy: 0.8843\n",
      "Epoch 105/200\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 1.3685 - accuracy: 0.6458 - categorical_accuracy: 0.6458 - top_k_categorical_accuracy: 0.8735\n",
      "Epoch 106/200\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1.3988 - accuracy: 0.6446 - categorical_accuracy: 0.6446 - top_k_categorical_accuracy: 0.8651\n",
      "Epoch 107/200\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1.3048 - accuracy: 0.6663 - categorical_accuracy: 0.6663 - top_k_categorical_accuracy: 0.8916\n",
      "Epoch 108/200\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1.3514 - accuracy: 0.6627 - categorical_accuracy: 0.6627 - top_k_categorical_accuracy: 0.8892\n",
      "Epoch 109/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1.3412 - accuracy: 0.6602 - categorical_accuracy: 0.6602 - top_k_categorical_accuracy: 0.8880\n",
      "Epoch 110/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1.3007 - accuracy: 0.6494 - categorical_accuracy: 0.6494 - top_k_categorical_accuracy: 0.8867\n",
      "Epoch 111/200\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1.2929 - accuracy: 0.6494 - categorical_accuracy: 0.6494 - top_k_categorical_accuracy: 0.8867\n",
      "Epoch 112/200\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1.3542 - accuracy: 0.6386 - categorical_accuracy: 0.6386 - top_k_categorical_accuracy: 0.8892\n",
      "Epoch 113/200\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1.3554 - accuracy: 0.6325 - categorical_accuracy: 0.6325 - top_k_categorical_accuracy: 0.8807\n",
      "Epoch 114/200\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1.3777 - accuracy: 0.6386 - categorical_accuracy: 0.6386 - top_k_categorical_accuracy: 0.8747\n",
      "Epoch 115/200\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1.3236 - accuracy: 0.6723 - categorical_accuracy: 0.6723 - top_k_categorical_accuracy: 0.8940\n",
      "Epoch 116/200\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1.3943 - accuracy: 0.6518 - categorical_accuracy: 0.6518 - top_k_categorical_accuracy: 0.8795\n",
      "Epoch 117/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1.3813 - accuracy: 0.6458 - categorical_accuracy: 0.6458 - top_k_categorical_accuracy: 0.8843\n",
      "Epoch 118/200\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1.4408 - accuracy: 0.6446 - categorical_accuracy: 0.6446 - top_k_categorical_accuracy: 0.8892\n",
      "Epoch 119/200\n",
      "166/166 [==============================] - 1s 4ms/step - loss: 1.2531 - accuracy: 0.6747 - categorical_accuracy: 0.6747 - top_k_categorical_accuracy: 0.8771\n",
      "Epoch 120/200\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1.3593 - accuracy: 0.6530 - categorical_accuracy: 0.6530 - top_k_categorical_accuracy: 0.8843\n",
      "Epoch 121/200\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 1.3650 - accuracy: 0.6470 - categorical_accuracy: 0.6470 - top_k_categorical_accuracy: 0.8723\n",
      "Epoch 122/200\n",
      "166/166 [==============================] - 1s 4ms/step - loss: 1.4216 - accuracy: 0.6373 - categorical_accuracy: 0.6373 - top_k_categorical_accuracy: 0.8831\n",
      "Epoch 123/200\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 1.3799 - accuracy: 0.6542 - categorical_accuracy: 0.6542 - top_k_categorical_accuracy: 0.8651\n",
      "Epoch 124/200\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1.3363 - accuracy: 0.6602 - categorical_accuracy: 0.6602 - top_k_categorical_accuracy: 0.8723\n",
      "Epoch 125/200\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1.3724 - accuracy: 0.6422 - categorical_accuracy: 0.6422 - top_k_categorical_accuracy: 0.8843\n",
      "Epoch 126/200\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1.4511 - accuracy: 0.6578 - categorical_accuracy: 0.6578 - top_k_categorical_accuracy: 0.8687\n",
      "Epoch 127/200\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 1.2835 - accuracy: 0.6554 - categorical_accuracy: 0.6554 - top_k_categorical_accuracy: 0.8916\n",
      "Epoch 128/200\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 1.5110 - accuracy: 0.6373 - categorical_accuracy: 0.6373 - top_k_categorical_accuracy: 0.8651\n",
      "Epoch 129/200\n",
      "166/166 [==============================] - 1s 4ms/step - loss: 1.5710 - accuracy: 0.6193 - categorical_accuracy: 0.6193 - top_k_categorical_accuracy: 0.8627\n",
      "Epoch 130/200\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 1.3977 - accuracy: 0.6566 - categorical_accuracy: 0.6566 - top_k_categorical_accuracy: 0.8747\n",
      "Epoch 131/200\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1.3570 - accuracy: 0.6494 - categorical_accuracy: 0.6494 - top_k_categorical_accuracy: 0.8747\n",
      "Epoch 132/200\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1.2851 - accuracy: 0.6614 - categorical_accuracy: 0.6614 - top_k_categorical_accuracy: 0.8964\n",
      "Epoch 133/200\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1.3296 - accuracy: 0.6723 - categorical_accuracy: 0.6723 - top_k_categorical_accuracy: 0.8807\n",
      "Epoch 134/200\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1.3024 - accuracy: 0.6627 - categorical_accuracy: 0.6627 - top_k_categorical_accuracy: 0.8795\n",
      "Epoch 135/200\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1.4148 - accuracy: 0.6398 - categorical_accuracy: 0.6398 - top_k_categorical_accuracy: 0.8759\n",
      "Epoch 136/200\n",
      "166/166 [==============================] - 1s 4ms/step - loss: 1.4237 - accuracy: 0.6482 - categorical_accuracy: 0.6482 - top_k_categorical_accuracy: 0.8711\n",
      "Epoch 137/200\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1.2961 - accuracy: 0.6747 - categorical_accuracy: 0.6747 - top_k_categorical_accuracy: 0.8928\n",
      "Epoch 138/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1.4085 - accuracy: 0.6398 - categorical_accuracy: 0.6398 - top_k_categorical_accuracy: 0.8976\n",
      "Epoch 139/200\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1.3960 - accuracy: 0.6518 - categorical_accuracy: 0.6518 - top_k_categorical_accuracy: 0.8807\n",
      "Epoch 140/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1.3819 - accuracy: 0.6578 - categorical_accuracy: 0.6578 - top_k_categorical_accuracy: 0.8855\n",
      "Epoch 141/200\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1.3988 - accuracy: 0.6566 - categorical_accuracy: 0.6566 - top_k_categorical_accuracy: 0.8867\n",
      "Epoch 142/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1.5207 - accuracy: 0.6458 - categorical_accuracy: 0.6458 - top_k_categorical_accuracy: 0.8639\n",
      "Epoch 143/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1.4590 - accuracy: 0.6434 - categorical_accuracy: 0.6434 - top_k_categorical_accuracy: 0.8795\n",
      "Epoch 144/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1.5041 - accuracy: 0.6410 - categorical_accuracy: 0.6410 - top_k_categorical_accuracy: 0.8687\n",
      "Epoch 145/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1.4923 - accuracy: 0.6470 - categorical_accuracy: 0.6470 - top_k_categorical_accuracy: 0.8590\n",
      "Epoch 146/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1.3456 - accuracy: 0.6723 - categorical_accuracy: 0.6723 - top_k_categorical_accuracy: 0.8952\n",
      "Epoch 147/200\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1.3679 - accuracy: 0.6590 - categorical_accuracy: 0.6590 - top_k_categorical_accuracy: 0.8867\n",
      "Epoch 148/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1.3638 - accuracy: 0.6470 - categorical_accuracy: 0.6470 - top_k_categorical_accuracy: 0.8831\n",
      "Epoch 149/200\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 1.2996 - accuracy: 0.6699 - categorical_accuracy: 0.6699 - top_k_categorical_accuracy: 0.8855\n",
      "Epoch 150/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1.4031 - accuracy: 0.6482 - categorical_accuracy: 0.6482 - top_k_categorical_accuracy: 0.8651\n",
      "Epoch 151/200\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1.4054 - accuracy: 0.6398 - categorical_accuracy: 0.6398 - top_k_categorical_accuracy: 0.8807\n",
      "Epoch 152/200\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1.4650 - accuracy: 0.6458 - categorical_accuracy: 0.6458 - top_k_categorical_accuracy: 0.8723\n",
      "Epoch 153/200\n",
      "166/166 [==============================] - 1s 4ms/step - loss: 1.3118 - accuracy: 0.6651 - categorical_accuracy: 0.6651 - top_k_categorical_accuracy: 0.8843\n",
      "Epoch 154/200\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1.3217 - accuracy: 0.6843 - categorical_accuracy: 0.6843 - top_k_categorical_accuracy: 0.8759\n",
      "Epoch 155/200\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1.4867 - accuracy: 0.6422 - categorical_accuracy: 0.6422 - top_k_categorical_accuracy: 0.8542\n",
      "Epoch 156/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1.3468 - accuracy: 0.6651 - categorical_accuracy: 0.6651 - top_k_categorical_accuracy: 0.8663\n",
      "Epoch 157/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1.4389 - accuracy: 0.6482 - categorical_accuracy: 0.6482 - top_k_categorical_accuracy: 0.8783\n",
      "Epoch 158/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1.3673 - accuracy: 0.6627 - categorical_accuracy: 0.6627 - top_k_categorical_accuracy: 0.8880\n",
      "Epoch 159/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1.4093 - accuracy: 0.6446 - categorical_accuracy: 0.6446 - top_k_categorical_accuracy: 0.8916\n",
      "Epoch 160/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1.3095 - accuracy: 0.6759 - categorical_accuracy: 0.6759 - top_k_categorical_accuracy: 0.8904\n",
      "Epoch 161/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1.3570 - accuracy: 0.6470 - categorical_accuracy: 0.6470 - top_k_categorical_accuracy: 0.8831\n",
      "Epoch 162/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1.3136 - accuracy: 0.6639 - categorical_accuracy: 0.6639 - top_k_categorical_accuracy: 0.8892\n",
      "Epoch 163/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1.4007 - accuracy: 0.6699 - categorical_accuracy: 0.6699 - top_k_categorical_accuracy: 0.8723\n",
      "Epoch 164/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1.4804 - accuracy: 0.6494 - categorical_accuracy: 0.6494 - top_k_categorical_accuracy: 0.8506\n",
      "Epoch 165/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1.3998 - accuracy: 0.6530 - categorical_accuracy: 0.6530 - top_k_categorical_accuracy: 0.8783\n",
      "Epoch 166/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1.3909 - accuracy: 0.6482 - categorical_accuracy: 0.6482 - top_k_categorical_accuracy: 0.8711\n",
      "Epoch 167/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1.4546 - accuracy: 0.6482 - categorical_accuracy: 0.6482 - top_k_categorical_accuracy: 0.8614\n",
      "Epoch 168/200\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1.3782 - accuracy: 0.6735 - categorical_accuracy: 0.6735 - top_k_categorical_accuracy: 0.8880\n",
      "Epoch 169/200\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1.3311 - accuracy: 0.6651 - categorical_accuracy: 0.6651 - top_k_categorical_accuracy: 0.8904\n",
      "Epoch 170/200\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1.3980 - accuracy: 0.6470 - categorical_accuracy: 0.6470 - top_k_categorical_accuracy: 0.8723\n",
      "Epoch 171/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1.3579 - accuracy: 0.6663 - categorical_accuracy: 0.6663 - top_k_categorical_accuracy: 0.8771\n",
      "Epoch 172/200\n",
      "166/166 [==============================] - 1s 5ms/step - loss: 1.2877 - accuracy: 0.6663 - categorical_accuracy: 0.6663 - top_k_categorical_accuracy: 0.8855\n",
      "Epoch 173/200\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1.3180 - accuracy: 0.6663 - categorical_accuracy: 0.6663 - top_k_categorical_accuracy: 0.8880\n",
      "Epoch 174/200\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 1.3814 - accuracy: 0.6759 - categorical_accuracy: 0.6759 - top_k_categorical_accuracy: 0.8639\n",
      "Epoch 175/200\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1.4317 - accuracy: 0.6470 - categorical_accuracy: 0.6470 - top_k_categorical_accuracy: 0.8819\n",
      "Epoch 176/200\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1.4336 - accuracy: 0.6687 - categorical_accuracy: 0.6687 - top_k_categorical_accuracy: 0.8819\n",
      "Epoch 177/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1.3756 - accuracy: 0.6590 - categorical_accuracy: 0.6590 - top_k_categorical_accuracy: 0.8988\n",
      "Epoch 178/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1.4869 - accuracy: 0.6373 - categorical_accuracy: 0.6373 - top_k_categorical_accuracy: 0.8711\n",
      "Epoch 179/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1.3984 - accuracy: 0.6627 - categorical_accuracy: 0.6627 - top_k_categorical_accuracy: 0.8735\n",
      "Epoch 180/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1.3517 - accuracy: 0.6614 - categorical_accuracy: 0.6614 - top_k_categorical_accuracy: 0.8916\n",
      "Epoch 181/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1.4954 - accuracy: 0.6470 - categorical_accuracy: 0.6470 - top_k_categorical_accuracy: 0.8602\n",
      "Epoch 182/200\n",
      "166/166 [==============================] - 1s 4ms/step - loss: 1.4952 - accuracy: 0.6349 - categorical_accuracy: 0.6349 - top_k_categorical_accuracy: 0.8675\n",
      "Epoch 183/200\n",
      "166/166 [==============================] - 1s 4ms/step - loss: 1.5840 - accuracy: 0.6373 - categorical_accuracy: 0.6373 - top_k_categorical_accuracy: 0.8663\n",
      "Epoch 184/200\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1.4902 - accuracy: 0.6542 - categorical_accuracy: 0.6542 - top_k_categorical_accuracy: 0.8747\n",
      "Epoch 185/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1.3433 - accuracy: 0.6699 - categorical_accuracy: 0.6699 - top_k_categorical_accuracy: 0.8952\n",
      "Epoch 186/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1.5456 - accuracy: 0.6482 - categorical_accuracy: 0.6482 - top_k_categorical_accuracy: 0.8759\n",
      "Epoch 187/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1.5435 - accuracy: 0.6434 - categorical_accuracy: 0.6434 - top_k_categorical_accuracy: 0.8711\n",
      "Epoch 188/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1.4898 - accuracy: 0.6518 - categorical_accuracy: 0.6518 - top_k_categorical_accuracy: 0.8663\n",
      "Epoch 189/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1.5040 - accuracy: 0.6361 - categorical_accuracy: 0.6361 - top_k_categorical_accuracy: 0.8747\n",
      "Epoch 190/200\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 1.3834 - accuracy: 0.6771 - categorical_accuracy: 0.6771 - top_k_categorical_accuracy: 0.8831\n",
      "Epoch 191/200\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 1.4559 - accuracy: 0.6398 - categorical_accuracy: 0.6398 - top_k_categorical_accuracy: 0.8831\n",
      "Epoch 192/200\n",
      "166/166 [==============================] - 1s 4ms/step - loss: 1.7241 - accuracy: 0.6277 - categorical_accuracy: 0.6277 - top_k_categorical_accuracy: 0.8590\n",
      "Epoch 193/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1.3857 - accuracy: 0.6614 - categorical_accuracy: 0.6614 - top_k_categorical_accuracy: 0.8880\n",
      "Epoch 194/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1.4842 - accuracy: 0.6325 - categorical_accuracy: 0.6325 - top_k_categorical_accuracy: 0.8723\n",
      "Epoch 195/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1.5214 - accuracy: 0.6530 - categorical_accuracy: 0.6530 - top_k_categorical_accuracy: 0.8711\n",
      "Epoch 196/200\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1.3462 - accuracy: 0.6687 - categorical_accuracy: 0.6687 - top_k_categorical_accuracy: 0.8783\n",
      "Epoch 197/200\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 1.4296 - accuracy: 0.6602 - categorical_accuracy: 0.6602 - top_k_categorical_accuracy: 0.8711\n",
      "Epoch 198/200\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1.4107 - accuracy: 0.6699 - categorical_accuracy: 0.6699 - top_k_categorical_accuracy: 0.8807\n",
      "Epoch 199/200\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1.5247 - accuracy: 0.6446 - categorical_accuracy: 0.6446 - top_k_categorical_accuracy: 0.8795\n",
      "Epoch 200/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1.5905 - accuracy: 0.6398 - categorical_accuracy: 0.6398 - top_k_categorical_accuracy: 0.8639\n",
      "model created\n"
     ]
    }
   ],
   "source": [
    "#fitting and saving the model \n",
    "hist = model.fit(np.array(train_x), np.array(train_y), epochs=400, batch_size=5, verbose=1)\n",
    "model.save('model.h5', hist)\n",
    "print(\"model created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = model.evaluate(train_x[0:20], train_y[0:20], verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sklearn'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32me:\\Codes\\Major Project\\Mental-health-Chatbot\\IntentModel\\training\\training.ipynb Cell 10\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/e%3A/Codes/Major%20Project/Mental-health-Chatbot/IntentModel/training/training.ipynb#X13sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmetrics\u001b[39;00m \u001b[39mimport\u001b[39;00m confusion_matrix\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'sklearn'"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = hist.history['accuracy']\n",
    "val_acc = hist.history['categorical_accuracy']\n",
    "loss = hist.history['loss']\n",
    "val_loss = hist.history['top_k_categorical_accuracy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = {\n",
    "    \"acc\": acc,\n",
    "    \"val_acc\": val_acc,\n",
    "    \"loss\": loss,\n",
    "    \"top_k_categorical_accuracy\": val_loss\n",
    "}\n",
    "\n",
    "# Serializing json\n",
    "json_object = json.dumps(dictionary, indent=4)\n",
    " \n",
    "# Writing to sample.json\n",
    "with open(\"sample.json\", \"w\") as outfile:\n",
    "    outfile.write(json_object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'matplotlib'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32me:\\Codes\\Major Project\\Mental-health-Chatbot\\IntentModel\\training\\training.ipynb Cell 13\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/e%3A/Codes/Major%20Project/Mental-health-Chatbot/IntentModel/training/training.ipynb#X14sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mmatplotlib\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpyplot\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mplt\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/Codes/Major%20Project/Mental-health-Chatbot/IntentModel/training/training.ipynb#X14sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m epochs \u001b[39m=\u001b[39m \u001b[39mrange\u001b[39m(\u001b[39m1\u001b[39m, \u001b[39mlen\u001b[39m(acc) \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/Codes/Major%20Project/Mental-health-Chatbot/IntentModel/training/training.ipynb#X14sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m plt\u001b[39m.\u001b[39mplot(epochs, acc, \u001b[39m'\u001b[39m\u001b[39mbo\u001b[39m\u001b[39m'\u001b[39m, label\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mTraining acc\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'matplotlib'"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "epochs = range(1, len(acc) + 1)\n",
    "\n",
    "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "# plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "# plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "# plt.title('Training and validation loss')\n",
    "# plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c5c2a1cb3f7f6160aa571082c3edf6897403ba248133963500dc500635300624"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
