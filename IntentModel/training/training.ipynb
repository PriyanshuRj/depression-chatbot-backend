{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing libraries\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "import json\n",
    "import pickle\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout\n",
    "from keras.optimizers import SGD\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "830 documents\n",
      "210 classes ['Anger Management', 'Anxiety', 'Cognitive Restructuring', 'Coping Strategies', 'Coping with Stress', 'Coping with Trauma', 'Harm Reduction', 'Identifying a Drug User', 'Legal Issues', 'Meditation Practices', 'Need for Change', 'Nutrition and Wellness', 'Overcoming Shame', 'Peer Support', 'Personal Boundaries', 'Recovery from Drug Abuse', 'Relapse Prevention', 'Resilience Building', 'Self-Care Practices', 'Social Anxiety', 'Substance Education', 'Support Systems', 'What are the types of depression?', 'Withdrawal Symptoms', 'Withdrawal Symptoms Strategies', 'about', 'academic_goal_setting', 'academic_performance', 'addiction', 'afternoon', 'anti_racism_resources', 'anxious', 'ask', 'at what age does anxiety peak?', 'body_dysmorphia_support', 'building_resilience', 'burden', 'can lack of sleep make you feel sad?', 'can low blood sugar cause suicidal thoughts?', 'casual', 'connecting_with_peers', 'coping_strategies', 'creation', 'creative_expression', 'daily_routines', 'death', 'default', 'depressed', 'depression_intro', 'do we control our thoughts?', 'does oversleeping cause depression?', 'done', 'drug use Patterns', 'encouragement', 'encouraging_activity', 'encouraging_self_help', 'evening', 'exploring_hobbies', 'fact-1', 'fact-10', 'fact-11', 'fact-12', 'fact-13', 'fact-14', 'fact-15', 'fact-16', 'fact-17', 'fact-18', 'fact-19', 'fact-2', 'fact-20', 'fact-21', 'fact-22', 'fact-23', 'fact-24', 'fact-25', 'fact-26', 'fact-27', 'fact-28', 'fact-29', 'fact-3', 'fact-30', 'fact-31', 'fact-32', 'fact-5', 'fact-6', 'fact-7', 'fact-8', 'fact-9', 'family_support', 'finding_sports_passion', 'friends', 'goodbye', 'greeting', 'happy', 'hate-me', 'hate-you', 'help', 'how can we reduce anxiety?', 'how does depression affect the world?', 'how long can anxiety last?', 'how many thoughts a day do we have?', 'i am a victim of bullying', 'i am afraid i will fail again', 'i am afraid to file a case against bullying', 'i am feeling anxious lately.', 'i am feeling stressed lately', 'i am good for nothing!', 'i am good for nothing.', 'i am lonely!', 'i am sad', 'i am stressed out', \"i can't do this anymore\", 'i feel i have let my parents down', 'i hate losing.', 'i hate myself!', 'i let everyojokne down', 'i think i am ugly!', \"i think i'm losing my mind\", 'i want a break', 'i want to kill myself', 'i want to leave the cou ntry and run away', 'i will never succeed in life', \"i wish i could've been a winner\", 'i wish i was better than them', 'i wish to quit', 'increment', 'is depression a side effect of diabetes?', 'is school a cause of depression?', 'jokes', 'learn-mental-health', 'learn-more', 'location', 'loneliness', 'media_influence_body_image', 'medication', 'meditation', 'mental-health-fact', 'mindfulness', 'morning', 'motivation', 'my time has come', 'neutral-response', 'night', 'no one likes me!', 'no-approach', 'no-response', 'no_girlfriend_boyfriend_loneliness', 'not-talking', 'nutrition_and_depression', 'overcoming_setbacks', 'pandora-useful', 'positive_affirmation', 'positive_body_image', 'positive_outlook', 'problem', 'promotion_denied', 'promotion_nervousness', 'promotion_seek', 'racism_experiences', 'recognizing_progress', 'relationship_expectations', 'relationships', 'repeat', 'reporting_racism', 'sad', 'scared', 'seeking_help', 'seeking_professional_help', 'self_assessment', 'self_care', 'self_harm', 'self_love_before_relationship', 'skill', 'sleep', 'sleep_and_depression', 'social_connection', 'something-else', 'sports_injury_recovery', 'sports_motivation', 'stress_due_to_marks', 'stress_management', 'stressed', 'struggle drug abuse', 'stupid', 'suicide', 'support_system', 'thanks', 'therapy', 'tracking_mood', 'understand', 'unplaced', 'user-advice', 'user-agree', 'user-meditation', 'what are the causes of depression?', 'what are the stages of anxiety?', 'what are the top causes of depression?', 'what is depression?', 'what is the 3 3 3 rule for anxiety?', 'what is the biological cause of depression?', 'what is the meaning of anxiety and depression?', 'which age group has the highest rate of depression?', 'which country has the highest rate of depression?', 'which country has the lowest rate of depression?', 'which race has the highest rate of depression?', 'why is anxiety bad for you?', 'work_and_depression', 'worthless', 'wrong']\n",
      "883 unique lemmatized words [\"'ll\", \"'m\", \"'re\", \"'s\", \"'ve\", ',', '.', '3', 'a', 'about', 'absolutely', 'abuse', 'academic', 'achieving', 'acknowledging', 'action', 'activity', 'addiction', 'address', 'advancement', 'advancing', 'advice', 'advisor', 'affect', 'affecting', 'afraid', 'after', 'afternoon', 'again', 'against', 'age', 'aid', 'alcohol', 'all', 'alleviate', 'alleviating', 'alone', 'alot', 'already', 'also', 'am', 'an', 'and', 'anger', 'another', 'answer', 'anti-racism', 'antidepressant', 'anuone', 'anxiety', 'anxious', 'any', 'anymore', 'anyone', 'anything', 'appearance', 'appears', 'approach', 'approaching', 'are', 'art', 'article', 'ashamed', 'ask', 'ass', 'assist', 'assistance', 'associated', 'at', 'attack', 'au', 'available', 'avoid', 'avoiding', 'away', 'awful', 'back', 'bad', 'balance', 'balancing', 'basis', 'battling', 'be', 'bearable', 'beauty', 'because', 'become', 'becomes', 'becoming', 'been', 'before', 'begin', 'behavior', 'behavioral', 'behind', 'benefit', 'best', 'better', 'between', 'bias', 'biological', 'blood', 'body', 'bonjour', 'book', 'both', 'bounce', 'boundary', 'boyfriend', 'break', 'breaking', 'breathe', 'bring', 'brother', 'build', 'building', 'bullying', 'burden', 'burned', 'but', 'by', 'bye', 'ca', 'call', 'calming', 'can', 'care', 'career', 'case', 'cause', 'caused', 'causing', 'celebrate', 'celebrating', 'challenge', 'challenging', 'change', 'chapter', 'charge', 'chat', 'check', 'cheerful', 'child', 'classmate', 'close', 'cognitive', 'college', 'come', 'commit', 'commitment', 'committed', 'common', 'communicate', 'community', 'companionship', 'concern', 'concerned', 'confidence', 'connect', 'connection', 'consequence', 'consider', 'considering', 'consult', 'continue', 'contribute', 'control', 'cope', 'coping', 'cou', 'could', 'counseling', 'counselor', 'country', 'craving', 'crazy', 'create', 'created', 'creating', 'creative', 'creativity', 'crisis', 'crucial', 'cue', 'cultivate', 'cultivating', 'cure', 'curious', 'cycle', 'dad', 'daily', 'dangerous', 'dating', 'day', 'deal', 'dealing', 'decided', 'define', 'denied', 'dependency', 'depressed', 'depression', 'depressive', 'deserve', 'determined', 'diabetes', 'did', 'die', 'died', 'dietary', 'difference', 'different', 'differentiate', 'difficult', 'disappointment', 'discomfort', 'disconnected', 'discouraged', 'discovering', 'discrimination', 'disorder', 'distorted', 'distract', 'disturbance', 'do', 'doe', 'doing', 'down', 'drinking', 'drug', 'drug-free', 'due', 'dumb', 'during', 'dysmorphia', 'ease', 'easier', 'eating', 'educate', 'educating', 'educational', 'effect', 'effective', 'effectively', 'else', 'embracing', 'emotional', 'emotionally', 'employ', 'employment', 'empowering', 'empty', 'encountering', 'encourage', 'end', 'engage', 'enjoy', 'enough', 'environment', 'episode', 'establish', 'establishing', 'evening', 'everting', 'everyday', 'everyojokne', 'everyone', 'everything', 'exam', 'exercise', 'exist', 'expect', 'expectation', 'experience', 'experienced', 'experiencing', 'explore', 'exploring', 'expression', 'face', 'facing', 'fact', 'fail', 'failure', 'family', 'fare', 'fear', 'feel', 'feeling', 'few', 'file', 'finance', 'financial', 'find', 'finding', 'fine', 'first', 'focus', 'for', 'forgeting', 'form', 'forward', 'fragile', 'free', 'fresh', 'friend', 'from', 'future', 'general', 'get', 'getting', 'girlfriend', 'girlfriend/boyfriend', 'give', 'go', 'goal', 'going', 'good', 'goodbye', 'grade', 'great', 'group', 'guess', 'guidance', 'guide', 'guilt', 'ha', 'habit', 'hack', 'had', 'hand', 'handle', 'handling', 'happy', 'hard', 'harm', 'hate', 'haulting', 'have', 'having', 'hay', 'healing', 'health', 'healthier', 'healthy', 'hello', 'help', 'helpful', 'helpline', 'here', 'hey', 'hi', 'highest', 'hinders', 'hmmm', 'hobby', 'hola', 'holistic', 'home', 'hope', 'hopeless', 'how', 'howdy', 'i', 'identify', 'if', 'ill', 'illness', 'image', 'immediate', 'impact', 'impending', 'importance', 'important', 'improve', 'improvement', 'improving', 'in', 'incident', 'include', 'incorporating', 'increment', 'indicate', 'indicating', 'indicator', 'individual', 'influence', 'information', 'initiate', 'injury', 'insight', 'insominia', 'insomnia', 'intensity', 'interest', 'interested', 'interpersonal', 'into', 'involve', 'involved', 'involving', 'is', 'isolated', 'isolation', 'issue', 'it', 'job', 'joke', 'journal', 'journey', 'joy', 'just', 'k', 'kill', 'killing', 'know', 'lack', 'last', 'lately', 'later', 'learn', 'learning', 'leave', 'left', 'legal', 'let', 'life', 'lifestyle', 'like', 'live', 'living', 'location', 'loneliness', 'lonely', 'long', 'long-term', 'look', 'looking', 'losing', 'lost', 'lot', 'love', 'loved', 'low', 'lowest', 'made', 'maintain', 'maintaining', 'make', 'manage', 'manageable', 'management', 'managing', 'many', 'mark', 'may', 'me', 'mean', 'meaning', 'meaningful', 'measuring', 'mechanism', 'media-driven', 'medication', 'meditation', 'medium', 'meeting', 'member', 'mental', 'mentally', 'mentioned', 'mentor', 'midst', 'might', 'milestone', 'mind', 'mindfulness', 'minimize', 'misuse', 'mitigate', 'mom', 'moment', 'money', 'monitoring', 'mood', 'more', 'morning', 'most', 'motivated', 'motivation', 'moving', 'much', 'my', 'myself', \"n't\", 'name', 'natural', 'navigate', 'need', 'needed', 'negatively', 'nervous', 'nervousness', 'network', 'never', 'new', 'next', 'nice', 'night', 'no', 'nobody', 'normal', 'not', 'nothing', 'now', 'ntry', 'nutrition', 'nutritional', 'obstacle', 'of', 'offer', 'often', 'oh', 'ok', 'okay', 'on', 'once', 'one', 'online', 'only', 'open', 'optimism', 'option', 'or', 'other', 'others', 'our', 'out', 'outlook', 'overcome', 'overcoming', 'oversleeping', 'overwhelmed', 'overwhelming', 'own', 'pain', 'panic', 'parent', 'passed', 'passion', 'past', 'path', 'pattern', 'peak', 'peer', 'people', 'performance', 'person', 'personal', 'personalized', 'physical', 'placed', 'plan', 'platform', 'play', 'please', 'portrayal', 'positive', 'possibly', 'practical', 'practice', 'practicing', 'prepared', 'preparing', 'pressure', 'prevent', 'preventing', 'prevention', 'prioritize', 'probably', 'problem', 'proceed', 'process', 'professional', 'program', 'progress', 'prolonged', 'promoted', 'promotion', 'proper', 'protect', 'provide', 'provides', 'public', 'purpose', 'quit', 'quitting', 'race', 'racial', 'racism', 'racist', 'rate', 'reach', 'reaching', 'ready', 'realistic', 'really', 'rebuilding', 'recognize', 'recognizing', 'recommend', 'recommendation', 'recover', 'recovering', 'recovery', 'reduce', 'reducing', 'reduction', 'regain', 'regarding', 'rejection', 'rekindling', 'relapse', 'relapsing', 'related', 'relationship', 'relax', 'relaxation', 'reliance', 'relief', 'remedy', 'remember', 'repeating', 'repercussion', 'report', 'reporting', 'requires', 'resilience', 'resilience-building', 'resistance', 'resource', 'response', 'responsibility', 'restructure', 'restructuring', 'returning', 'revoir', 'right', 'robot', 'role', 'routine', 'rule', 'run', 'sad', 'sadness', 'safe', 'said', 'sasa', 'say', 'saying', 'sayonara', 'scared', 'schedule', 'school', 'search', 'see', 'seek', 'seeking', 'seem', 'seems', 'self-assessment', 'self-care', 'self-destructive', 'self-esteem', 'self-harm', 'self-harming', 'self-help', 'self-love', 'self-worth', 'senior', 'sense', 'serious', 'service', 'set', 'setback', 'setting', 'severity', 'shame', 'share', 'shared', 'sharing', 'should', 'shut', 'shyness', 'side', 'sign', 'significant', 'singlehood', 'sister', 'situation', 'sleep', 'slept', 'small', 'smaller', 'smoking', 'smoother', 'so', 'sober', 'sobriety', 'social', 'societal', 'solidarity', 'some', 'someone', 'something', 'sound', 'specific', 'sport', 'spot', 'stage', 'standard', 'start', 'started', 'starting', 'state', 'stay', 'step', 'still', 'story', 'strategy', 'strength', 'strengthen', 'stress', 'stressed', 'stressful', 'structure', 'struggle', 'struggling', 'stuck', 'student', 'study', 'stupid', 'subside', 'substance', 'succeed', 'success', 'suffering', 'sugar', 'suggest', 'suggestion', 'suicidal', 'suicide', 'support', 'supporting', 'supportive', 'sure', 'suspect', 'symptom', 'system', 'tailored', 'take', 'taking', 'talk', 'talking', 'task', 'teacher', 'team', 'technique', 'tell', 'tempted', 'than', 'thank', 'thanks', 'that', 'the', 'thee', 'their', 'them', 'then', 'therapist', 'therapy', 'there', 'these', 'thing', 'think', 'thinking', 'this', 'thought', 'through', 'time', 'timeline', 'tip', 'tired', 'to', 'today', 'told', 'too', 'tool', 'top', 'tough', 'toughness', 'track', 'tracking', 'transform', 'transition', 'trauma', 'treating', 'treatment', 'trigger', 'trust', 'trusted', 'trying', 'turn', 'turning', 'type', 'typical', 'ugly', 'understand', 'understanding', 'understands', 'unemployed', 'unmotivated', 'unplaced', 'unrealistic', 'unsupported', 'unwell', 'up', 'urge', 'use', 'used', 'useful', 'useless', 'using', 'usually', 'very', 'victim', 'victory', 'visit', 'wa', 'want', 'warning', 'watch', 'way', 'we', 'website', 'weight', 'well', 'well-being', 'wellness', 'were', 'what', 'whatever', 'when', 'where', 'which', 'while', 'who', 'why', 'will', 'winner', 'wish', 'with', 'withdrawal', 'without', 'work', 'work-life', 'work-related', 'workload', 'workout', 'workplace', 'world', 'worried', 'worry', 'worsening', 'worthless', 'would', 'wrong', 'yeah', 'yes', 'yet', 'you', 'your', 'yourself']\n"
     ]
    }
   ],
   "source": [
    "# tokenizing and lematizing\n",
    "words=[]\n",
    "classes = []\n",
    "documents = []\n",
    "ignore_words = ['?', '!']\n",
    "data_file = open('intents.json').read()\n",
    "intents = json.loads(data_file)\n",
    "\n",
    "for intent in intents['intents']:\n",
    "    for pattern in intent['patterns']:\n",
    "        #tokenize each word\n",
    "        w = nltk.word_tokenize(pattern)\n",
    "        words.extend(w)\n",
    "        #add documents in the corpus\n",
    "        documents.append((w, intent['tag']))\n",
    "        # add to our classes list\n",
    "        if intent['tag'] not in classes:\n",
    "            classes.append(intent['tag'])\n",
    "# lemmatize and lower each word and remove duplicates\n",
    "words = [lemmatizer.lemmatize(w.lower()) for w in words if w not in ignore_words]\n",
    "words = sorted(list(set(words)))\n",
    "# sort classes\n",
    "classes = sorted(list(set(classes)))\n",
    "# documents = combination between patterns and intents\n",
    "print (len(documents), \"documents\")\n",
    "# classes = intents\n",
    "print (len(classes), \"classes\", classes)\n",
    "# words = all words, vocabulary\n",
    "print (len(words), \"unique lemmatized words\", words)\n",
    "pickle.dump(words,open('texts.pkl','wb'))\n",
    "pickle.dump(classes,open('labels.pkl','wb'))\n",
    "# create our training data\n",
    "training = []\n",
    "# create an empty array for our output\n",
    "output_empty = [0] * len(classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# training set, bag of words for each sentence\n",
    "for doc in documents:\n",
    "    # initialize our bag of words\n",
    "    bag = []\n",
    "    # list of tokenized words for the pattern\n",
    "    pattern_words = doc[0]\n",
    "    # lemmatize each word - create base word, in attempt to represent related words\n",
    "    pattern_words = [lemmatizer.lemmatize(word.lower()) for word in pattern_words]\n",
    "    # create our bag of words array with 1, if word match found in current pattern\n",
    "    for w in words:\n",
    "        bag.append(1) if w in pattern_words else bag.append(0)\n",
    "    \n",
    "    # output is a '0' for each tag and '1' for current tag (for each pattern)\n",
    "    output_row = list(output_empty)\n",
    "    output_row[classes.index(doc[1])] = 1\n",
    "    \n",
    "    training.append([bag, output_row])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "830\n"
     ]
    }
   ],
   "source": [
    "# shuffle our features and turn into np.array\n",
    "print(len(training))\n",
    "random.shuffle(training)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data created\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# training = np.array(training)\n",
    "# create train and test lists. X - patterns, Y - intents\n",
    "train_x = []\n",
    "train_y = []\n",
    "for i in training:\n",
    "    train_x.append(i[0])\n",
    "    train_y.append(i[1])\n",
    "# train_x = list(training[:,0])\n",
    "# train_y = list(training[:,1])\n",
    "print(\"Training data created\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model - 3 layers. First layer 128 neurons, second layer 64 neurons and 3rd output layer contains number of neurons\n",
    "# equal to number of intents to predict output intent with softmax\n",
    "model = Sequential()\n",
    "model.add(Dense(128, input_shape=(len(train_x[0]),), activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(len(train_y[0]), activation='softmax'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.SGD.\n"
     ]
    }
   ],
   "source": [
    "# Compile model. Stochastic gradient descent with Nesterov accelerated gradient gives good results for this model\n",
    "sgd = SGD(lr=0.01,  momentum=0.9, nesterov=True)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy', 'categorical_accuracy', 'top_k_categorical_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/400\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 1.5153 - accuracy: 0.6578 - categorical_accuracy: 0.6578 - top_k_categorical_accuracy: 0.8771\n",
      "Epoch 2/400\n",
      "166/166 [==============================] - 1s 5ms/step - loss: 1.3943 - accuracy: 0.6663 - categorical_accuracy: 0.6663 - top_k_categorical_accuracy: 0.8831\n",
      "Epoch 3/400\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 1.5062 - accuracy: 0.6518 - categorical_accuracy: 0.6518 - top_k_categorical_accuracy: 0.8747\n",
      "Epoch 4/400\n",
      "166/166 [==============================] - 1s 4ms/step - loss: 1.6473 - accuracy: 0.6458 - categorical_accuracy: 0.6458 - top_k_categorical_accuracy: 0.8651\n",
      "Epoch 5/400\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 1.4789 - accuracy: 0.6446 - categorical_accuracy: 0.6446 - top_k_categorical_accuracy: 0.8675\n",
      "Epoch 6/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1.5960 - accuracy: 0.6169 - categorical_accuracy: 0.6169 - top_k_categorical_accuracy: 0.8542\n",
      "Epoch 7/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1.6284 - accuracy: 0.6289 - categorical_accuracy: 0.6289 - top_k_categorical_accuracy: 0.8506\n",
      "Epoch 8/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1.5527 - accuracy: 0.6663 - categorical_accuracy: 0.6663 - top_k_categorical_accuracy: 0.8602\n",
      "Epoch 9/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1.4884 - accuracy: 0.6566 - categorical_accuracy: 0.6566 - top_k_categorical_accuracy: 0.8627\n",
      "Epoch 10/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1.4009 - accuracy: 0.6627 - categorical_accuracy: 0.6627 - top_k_categorical_accuracy: 0.8639\n",
      "Epoch 11/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1.4356 - accuracy: 0.6578 - categorical_accuracy: 0.6578 - top_k_categorical_accuracy: 0.8675\n",
      "Epoch 12/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1.6208 - accuracy: 0.6554 - categorical_accuracy: 0.6554 - top_k_categorical_accuracy: 0.8482\n",
      "Epoch 13/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1.5238 - accuracy: 0.6398 - categorical_accuracy: 0.6398 - top_k_categorical_accuracy: 0.8590\n",
      "Epoch 14/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1.5815 - accuracy: 0.6494 - categorical_accuracy: 0.6494 - top_k_categorical_accuracy: 0.8663\n",
      "Epoch 15/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1.4584 - accuracy: 0.6711 - categorical_accuracy: 0.6711 - top_k_categorical_accuracy: 0.8687\n",
      "Epoch 16/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1.4707 - accuracy: 0.6627 - categorical_accuracy: 0.6627 - top_k_categorical_accuracy: 0.8663\n",
      "Epoch 17/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1.4356 - accuracy: 0.6687 - categorical_accuracy: 0.6687 - top_k_categorical_accuracy: 0.8735\n",
      "Epoch 18/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1.4264 - accuracy: 0.6566 - categorical_accuracy: 0.6566 - top_k_categorical_accuracy: 0.8711\n",
      "Epoch 19/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1.3966 - accuracy: 0.6590 - categorical_accuracy: 0.6590 - top_k_categorical_accuracy: 0.8783\n",
      "Epoch 20/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1.3717 - accuracy: 0.6843 - categorical_accuracy: 0.6843 - top_k_categorical_accuracy: 0.8831\n",
      "Epoch 21/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1.3806 - accuracy: 0.6590 - categorical_accuracy: 0.6590 - top_k_categorical_accuracy: 0.8759\n",
      "Epoch 22/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1.4192 - accuracy: 0.6578 - categorical_accuracy: 0.6578 - top_k_categorical_accuracy: 0.8747\n",
      "Epoch 23/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1.3060 - accuracy: 0.6795 - categorical_accuracy: 0.6795 - top_k_categorical_accuracy: 0.8916\n",
      "Epoch 24/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1.5532 - accuracy: 0.6578 - categorical_accuracy: 0.6578 - top_k_categorical_accuracy: 0.8759\n",
      "Epoch 25/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1.5115 - accuracy: 0.6506 - categorical_accuracy: 0.6506 - top_k_categorical_accuracy: 0.8675\n",
      "Epoch 26/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1.6409 - accuracy: 0.6289 - categorical_accuracy: 0.6289 - top_k_categorical_accuracy: 0.8602\n",
      "Epoch 27/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1.4963 - accuracy: 0.6602 - categorical_accuracy: 0.6602 - top_k_categorical_accuracy: 0.8723\n",
      "Epoch 28/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1.5540 - accuracy: 0.6349 - categorical_accuracy: 0.6349 - top_k_categorical_accuracy: 0.8663\n",
      "Epoch 29/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1.6416 - accuracy: 0.6337 - categorical_accuracy: 0.6337 - top_k_categorical_accuracy: 0.8530\n",
      "Epoch 30/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1.4351 - accuracy: 0.6651 - categorical_accuracy: 0.6651 - top_k_categorical_accuracy: 0.8795\n",
      "Epoch 31/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1.5650 - accuracy: 0.6482 - categorical_accuracy: 0.6482 - top_k_categorical_accuracy: 0.8566\n",
      "Epoch 32/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1.6266 - accuracy: 0.6578 - categorical_accuracy: 0.6578 - top_k_categorical_accuracy: 0.8651\n",
      "Epoch 33/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1.5718 - accuracy: 0.6386 - categorical_accuracy: 0.6386 - top_k_categorical_accuracy: 0.8518\n",
      "Epoch 34/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1.5751 - accuracy: 0.6349 - categorical_accuracy: 0.6349 - top_k_categorical_accuracy: 0.8759\n",
      "Epoch 35/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1.4909 - accuracy: 0.6410 - categorical_accuracy: 0.6410 - top_k_categorical_accuracy: 0.8530\n",
      "Epoch 36/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1.5179 - accuracy: 0.6458 - categorical_accuracy: 0.6458 - top_k_categorical_accuracy: 0.8627\n",
      "Epoch 37/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1.6718 - accuracy: 0.6036 - categorical_accuracy: 0.6036 - top_k_categorical_accuracy: 0.8494\n",
      "Epoch 38/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1.4284 - accuracy: 0.6747 - categorical_accuracy: 0.6747 - top_k_categorical_accuracy: 0.8602\n",
      "Epoch 39/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1.4557 - accuracy: 0.6458 - categorical_accuracy: 0.6458 - top_k_categorical_accuracy: 0.8795\n",
      "Epoch 40/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1.5987 - accuracy: 0.6325 - categorical_accuracy: 0.6325 - top_k_categorical_accuracy: 0.8602\n",
      "Epoch 41/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1.5682 - accuracy: 0.6554 - categorical_accuracy: 0.6554 - top_k_categorical_accuracy: 0.8747\n",
      "Epoch 42/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1.4914 - accuracy: 0.6627 - categorical_accuracy: 0.6627 - top_k_categorical_accuracy: 0.8590\n",
      "Epoch 43/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1.5768 - accuracy: 0.6554 - categorical_accuracy: 0.6554 - top_k_categorical_accuracy: 0.8602\n",
      "Epoch 44/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1.5811 - accuracy: 0.6578 - categorical_accuracy: 0.6578 - top_k_categorical_accuracy: 0.8566\n",
      "Epoch 45/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1.6652 - accuracy: 0.6458 - categorical_accuracy: 0.6458 - top_k_categorical_accuracy: 0.8566\n",
      "Epoch 46/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1.6610 - accuracy: 0.6373 - categorical_accuracy: 0.6373 - top_k_categorical_accuracy: 0.8759\n",
      "Epoch 47/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1.5464 - accuracy: 0.6506 - categorical_accuracy: 0.6506 - top_k_categorical_accuracy: 0.8602\n",
      "Epoch 48/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1.5698 - accuracy: 0.6554 - categorical_accuracy: 0.6554 - top_k_categorical_accuracy: 0.8627\n",
      "Epoch 49/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1.5511 - accuracy: 0.6337 - categorical_accuracy: 0.6337 - top_k_categorical_accuracy: 0.8482\n",
      "Epoch 50/400\n",
      "166/166 [==============================] - 1s 4ms/step - loss: 1.7013 - accuracy: 0.6361 - categorical_accuracy: 0.6361 - top_k_categorical_accuracy: 0.8699\n",
      "Epoch 51/400\n",
      "166/166 [==============================] - 1s 4ms/step - loss: 1.8005 - accuracy: 0.6145 - categorical_accuracy: 0.6145 - top_k_categorical_accuracy: 0.8470\n",
      "Epoch 52/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1.4638 - accuracy: 0.6795 - categorical_accuracy: 0.6795 - top_k_categorical_accuracy: 0.8663\n",
      "Epoch 53/400\n",
      "166/166 [==============================] - 1s 4ms/step - loss: 1.8688 - accuracy: 0.6000 - categorical_accuracy: 0.6000 - top_k_categorical_accuracy: 0.8337\n",
      "Epoch 54/400\n",
      "166/166 [==============================] - 1s 4ms/step - loss: 1.5088 - accuracy: 0.6566 - categorical_accuracy: 0.6566 - top_k_categorical_accuracy: 0.8614\n",
      "Epoch 55/400\n",
      "166/166 [==============================] - 1s 4ms/step - loss: 1.7901 - accuracy: 0.6048 - categorical_accuracy: 0.6048 - top_k_categorical_accuracy: 0.8639\n",
      "Epoch 56/400\n",
      "166/166 [==============================] - 1s 4ms/step - loss: 1.6210 - accuracy: 0.6313 - categorical_accuracy: 0.6313 - top_k_categorical_accuracy: 0.8578\n",
      "Epoch 57/400\n",
      "166/166 [==============================] - 1s 4ms/step - loss: 1.5665 - accuracy: 0.6337 - categorical_accuracy: 0.6337 - top_k_categorical_accuracy: 0.8639\n",
      "Epoch 58/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1.5391 - accuracy: 0.6373 - categorical_accuracy: 0.6373 - top_k_categorical_accuracy: 0.8506\n",
      "Epoch 59/400\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 1.4894 - accuracy: 0.6663 - categorical_accuracy: 0.6663 - top_k_categorical_accuracy: 0.8651\n",
      "Epoch 60/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1.6559 - accuracy: 0.6398 - categorical_accuracy: 0.6398 - top_k_categorical_accuracy: 0.8590\n",
      "Epoch 61/400\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 1.6251 - accuracy: 0.6422 - categorical_accuracy: 0.6422 - top_k_categorical_accuracy: 0.8627\n",
      "Epoch 62/400\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 1.5187 - accuracy: 0.6446 - categorical_accuracy: 0.6446 - top_k_categorical_accuracy: 0.8614\n",
      "Epoch 63/400\n",
      "166/166 [==============================] - 1s 5ms/step - loss: 1.6834 - accuracy: 0.6337 - categorical_accuracy: 0.6337 - top_k_categorical_accuracy: 0.8627\n",
      "Epoch 64/400\n",
      "166/166 [==============================] - 1s 4ms/step - loss: 1.6698 - accuracy: 0.6205 - categorical_accuracy: 0.6205 - top_k_categorical_accuracy: 0.8313\n",
      "Epoch 65/400\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 1.6298 - accuracy: 0.6410 - categorical_accuracy: 0.6410 - top_k_categorical_accuracy: 0.8446\n",
      "Epoch 66/400\n",
      "166/166 [==============================] - 1s 4ms/step - loss: 1.7633 - accuracy: 0.6193 - categorical_accuracy: 0.6193 - top_k_categorical_accuracy: 0.8458\n",
      "Epoch 67/400\n",
      "166/166 [==============================] - 1s 4ms/step - loss: 1.4673 - accuracy: 0.6410 - categorical_accuracy: 0.6410 - top_k_categorical_accuracy: 0.8566\n",
      "Epoch 68/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1.6148 - accuracy: 0.6530 - categorical_accuracy: 0.6530 - top_k_categorical_accuracy: 0.8506\n",
      "Epoch 69/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1.6443 - accuracy: 0.6337 - categorical_accuracy: 0.6337 - top_k_categorical_accuracy: 0.8458\n",
      "Epoch 70/400\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 1.5035 - accuracy: 0.6518 - categorical_accuracy: 0.6518 - top_k_categorical_accuracy: 0.8542\n",
      "Epoch 71/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1.6454 - accuracy: 0.6277 - categorical_accuracy: 0.6277 - top_k_categorical_accuracy: 0.8494\n",
      "Epoch 72/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1.6613 - accuracy: 0.6325 - categorical_accuracy: 0.6325 - top_k_categorical_accuracy: 0.8494\n",
      "Epoch 73/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1.5102 - accuracy: 0.6602 - categorical_accuracy: 0.6602 - top_k_categorical_accuracy: 0.8723\n",
      "Epoch 74/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1.3799 - accuracy: 0.6675 - categorical_accuracy: 0.6675 - top_k_categorical_accuracy: 0.8747\n",
      "Epoch 75/400\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 1.4413 - accuracy: 0.6578 - categorical_accuracy: 0.6578 - top_k_categorical_accuracy: 0.8892\n",
      "Epoch 76/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1.6627 - accuracy: 0.6482 - categorical_accuracy: 0.6482 - top_k_categorical_accuracy: 0.8602\n",
      "Epoch 77/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1.7483 - accuracy: 0.6217 - categorical_accuracy: 0.6217 - top_k_categorical_accuracy: 0.8482\n",
      "Epoch 78/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1.6874 - accuracy: 0.6325 - categorical_accuracy: 0.6325 - top_k_categorical_accuracy: 0.8590\n",
      "Epoch 79/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 2.0634 - accuracy: 0.6169 - categorical_accuracy: 0.6169 - top_k_categorical_accuracy: 0.8265\n",
      "Epoch 80/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1.9709 - accuracy: 0.6133 - categorical_accuracy: 0.6133 - top_k_categorical_accuracy: 0.8386\n",
      "Epoch 81/400\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 1.8099 - accuracy: 0.6108 - categorical_accuracy: 0.6108 - top_k_categorical_accuracy: 0.8590\n",
      "Epoch 82/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1.7806 - accuracy: 0.6072 - categorical_accuracy: 0.6072 - top_k_categorical_accuracy: 0.8253\n",
      "Epoch 83/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1.7418 - accuracy: 0.6157 - categorical_accuracy: 0.6157 - top_k_categorical_accuracy: 0.8349\n",
      "Epoch 84/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1.7227 - accuracy: 0.6301 - categorical_accuracy: 0.6301 - top_k_categorical_accuracy: 0.8349\n",
      "Epoch 85/400\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 1.6723 - accuracy: 0.6181 - categorical_accuracy: 0.6181 - top_k_categorical_accuracy: 0.8458\n",
      "Epoch 86/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1.8907 - accuracy: 0.6169 - categorical_accuracy: 0.6169 - top_k_categorical_accuracy: 0.8494\n",
      "Epoch 87/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1.8005 - accuracy: 0.6072 - categorical_accuracy: 0.6072 - top_k_categorical_accuracy: 0.8241\n",
      "Epoch 88/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1.8029 - accuracy: 0.6422 - categorical_accuracy: 0.6422 - top_k_categorical_accuracy: 0.8386\n",
      "Epoch 89/400\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 1.6593 - accuracy: 0.6410 - categorical_accuracy: 0.6410 - top_k_categorical_accuracy: 0.8663\n",
      "Epoch 90/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1.6356 - accuracy: 0.6277 - categorical_accuracy: 0.6277 - top_k_categorical_accuracy: 0.8578\n",
      "Epoch 91/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1.8244 - accuracy: 0.5928 - categorical_accuracy: 0.5928 - top_k_categorical_accuracy: 0.8289\n",
      "Epoch 92/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1.7765 - accuracy: 0.6133 - categorical_accuracy: 0.6133 - top_k_categorical_accuracy: 0.8410\n",
      "Epoch 93/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1.8306 - accuracy: 0.6241 - categorical_accuracy: 0.6241 - top_k_categorical_accuracy: 0.8361\n",
      "Epoch 94/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1.6461 - accuracy: 0.6361 - categorical_accuracy: 0.6361 - top_k_categorical_accuracy: 0.8410\n",
      "Epoch 95/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1.8073 - accuracy: 0.6506 - categorical_accuracy: 0.6506 - top_k_categorical_accuracy: 0.8301\n",
      "Epoch 96/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1.6349 - accuracy: 0.6337 - categorical_accuracy: 0.6337 - top_k_categorical_accuracy: 0.8494\n",
      "Epoch 97/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1.7149 - accuracy: 0.6169 - categorical_accuracy: 0.6169 - top_k_categorical_accuracy: 0.8410\n",
      "Epoch 98/400\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 1.6671 - accuracy: 0.6313 - categorical_accuracy: 0.6313 - top_k_categorical_accuracy: 0.8530\n",
      "Epoch 99/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1.6385 - accuracy: 0.6410 - categorical_accuracy: 0.6410 - top_k_categorical_accuracy: 0.8506\n",
      "Epoch 100/400\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 1.7477 - accuracy: 0.6181 - categorical_accuracy: 0.6181 - top_k_categorical_accuracy: 0.8313\n",
      "Epoch 101/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1.8861 - accuracy: 0.6072 - categorical_accuracy: 0.6072 - top_k_categorical_accuracy: 0.8386\n",
      "Epoch 102/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1.5959 - accuracy: 0.6373 - categorical_accuracy: 0.6373 - top_k_categorical_accuracy: 0.8482\n",
      "Epoch 103/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1.6634 - accuracy: 0.6446 - categorical_accuracy: 0.6446 - top_k_categorical_accuracy: 0.8470\n",
      "Epoch 104/400\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 1.7598 - accuracy: 0.6193 - categorical_accuracy: 0.6193 - top_k_categorical_accuracy: 0.8337\n",
      "Epoch 105/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1.6384 - accuracy: 0.6373 - categorical_accuracy: 0.6373 - top_k_categorical_accuracy: 0.8313\n",
      "Epoch 106/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1.7141 - accuracy: 0.6205 - categorical_accuracy: 0.6205 - top_k_categorical_accuracy: 0.8361\n",
      "Epoch 107/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1.7826 - accuracy: 0.6301 - categorical_accuracy: 0.6301 - top_k_categorical_accuracy: 0.8482\n",
      "Epoch 108/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1.7372 - accuracy: 0.6446 - categorical_accuracy: 0.6446 - top_k_categorical_accuracy: 0.8470\n",
      "Epoch 109/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1.5473 - accuracy: 0.6542 - categorical_accuracy: 0.6542 - top_k_categorical_accuracy: 0.8518\n",
      "Epoch 110/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1.5251 - accuracy: 0.6578 - categorical_accuracy: 0.6578 - top_k_categorical_accuracy: 0.8530\n",
      "Epoch 111/400\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 1.6068 - accuracy: 0.6337 - categorical_accuracy: 0.6337 - top_k_categorical_accuracy: 0.8494\n",
      "Epoch 112/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1.5874 - accuracy: 0.6398 - categorical_accuracy: 0.6398 - top_k_categorical_accuracy: 0.8675\n",
      "Epoch 113/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1.6670 - accuracy: 0.6277 - categorical_accuracy: 0.6277 - top_k_categorical_accuracy: 0.8349\n",
      "Epoch 114/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1.5585 - accuracy: 0.6735 - categorical_accuracy: 0.6735 - top_k_categorical_accuracy: 0.8687\n",
      "Epoch 115/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1.6798 - accuracy: 0.6458 - categorical_accuracy: 0.6458 - top_k_categorical_accuracy: 0.8506\n",
      "Epoch 116/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1.5367 - accuracy: 0.6566 - categorical_accuracy: 0.6566 - top_k_categorical_accuracy: 0.8627\n",
      "Epoch 117/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1.6988 - accuracy: 0.6470 - categorical_accuracy: 0.6470 - top_k_categorical_accuracy: 0.8723\n",
      "Epoch 118/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1.7847 - accuracy: 0.6217 - categorical_accuracy: 0.6217 - top_k_categorical_accuracy: 0.8494\n",
      "Epoch 119/400\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 1.6933 - accuracy: 0.6446 - categorical_accuracy: 0.6446 - top_k_categorical_accuracy: 0.8554\n",
      "Epoch 120/400\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 1.6986 - accuracy: 0.6349 - categorical_accuracy: 0.6349 - top_k_categorical_accuracy: 0.8530\n",
      "Epoch 121/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1.5773 - accuracy: 0.6699 - categorical_accuracy: 0.6699 - top_k_categorical_accuracy: 0.8578\n",
      "Epoch 122/400\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 1.8244 - accuracy: 0.6301 - categorical_accuracy: 0.6301 - top_k_categorical_accuracy: 0.8446\n",
      "Epoch 123/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1.7888 - accuracy: 0.6084 - categorical_accuracy: 0.6084 - top_k_categorical_accuracy: 0.8277\n",
      "Epoch 124/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1.6125 - accuracy: 0.6422 - categorical_accuracy: 0.6422 - top_k_categorical_accuracy: 0.8554\n",
      "Epoch 125/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1.8339 - accuracy: 0.6181 - categorical_accuracy: 0.6181 - top_k_categorical_accuracy: 0.8434\n",
      "Epoch 126/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1.6984 - accuracy: 0.6301 - categorical_accuracy: 0.6301 - top_k_categorical_accuracy: 0.8289\n",
      "Epoch 127/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1.7829 - accuracy: 0.6181 - categorical_accuracy: 0.6181 - top_k_categorical_accuracy: 0.8398\n",
      "Epoch 128/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1.7728 - accuracy: 0.6265 - categorical_accuracy: 0.6265 - top_k_categorical_accuracy: 0.8506\n",
      "Epoch 129/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1.6809 - accuracy: 0.6277 - categorical_accuracy: 0.6277 - top_k_categorical_accuracy: 0.8434\n",
      "Epoch 130/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1.7253 - accuracy: 0.6229 - categorical_accuracy: 0.6229 - top_k_categorical_accuracy: 0.8578\n",
      "Epoch 131/400\n",
      "166/166 [==============================] - 1s 4ms/step - loss: 1.8212 - accuracy: 0.6277 - categorical_accuracy: 0.6277 - top_k_categorical_accuracy: 0.8241\n",
      "Epoch 132/400\n",
      "166/166 [==============================] - 1s 4ms/step - loss: 1.8513 - accuracy: 0.5988 - categorical_accuracy: 0.5988 - top_k_categorical_accuracy: 0.8494\n",
      "Epoch 133/400\n",
      "166/166 [==============================] - 1s 4ms/step - loss: 1.8517 - accuracy: 0.6048 - categorical_accuracy: 0.6048 - top_k_categorical_accuracy: 0.8373\n",
      "Epoch 134/400\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 1.7847 - accuracy: 0.6000 - categorical_accuracy: 0.6000 - top_k_categorical_accuracy: 0.8373\n",
      "Epoch 135/400\n",
      "166/166 [==============================] - 1s 4ms/step - loss: 1.7834 - accuracy: 0.6133 - categorical_accuracy: 0.6133 - top_k_categorical_accuracy: 0.8325\n",
      "Epoch 136/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1.5313 - accuracy: 0.6373 - categorical_accuracy: 0.6373 - top_k_categorical_accuracy: 0.8687\n",
      "Epoch 137/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1.7317 - accuracy: 0.6217 - categorical_accuracy: 0.6217 - top_k_categorical_accuracy: 0.8289\n",
      "Epoch 138/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1.7133 - accuracy: 0.6301 - categorical_accuracy: 0.6301 - top_k_categorical_accuracy: 0.8205\n",
      "Epoch 139/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1.6218 - accuracy: 0.6651 - categorical_accuracy: 0.6651 - top_k_categorical_accuracy: 0.8446\n",
      "Epoch 140/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1.7434 - accuracy: 0.6108 - categorical_accuracy: 0.6108 - top_k_categorical_accuracy: 0.8458\n",
      "Epoch 141/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1.7716 - accuracy: 0.6325 - categorical_accuracy: 0.6325 - top_k_categorical_accuracy: 0.8410\n",
      "Epoch 142/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1.5970 - accuracy: 0.6217 - categorical_accuracy: 0.6217 - top_k_categorical_accuracy: 0.8446\n",
      "Epoch 143/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1.6465 - accuracy: 0.6482 - categorical_accuracy: 0.6482 - top_k_categorical_accuracy: 0.8602\n",
      "Epoch 144/400\n",
      "166/166 [==============================] - 1s 4ms/step - loss: 1.5733 - accuracy: 0.6627 - categorical_accuracy: 0.6627 - top_k_categorical_accuracy: 0.8554\n",
      "Epoch 145/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1.6904 - accuracy: 0.6434 - categorical_accuracy: 0.6434 - top_k_categorical_accuracy: 0.8434\n",
      "Epoch 146/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1.7929 - accuracy: 0.6289 - categorical_accuracy: 0.6289 - top_k_categorical_accuracy: 0.8301\n",
      "Epoch 147/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1.6824 - accuracy: 0.6193 - categorical_accuracy: 0.6193 - top_k_categorical_accuracy: 0.8446\n",
      "Epoch 148/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1.7058 - accuracy: 0.6337 - categorical_accuracy: 0.6337 - top_k_categorical_accuracy: 0.8325\n",
      "Epoch 149/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1.8095 - accuracy: 0.6169 - categorical_accuracy: 0.6169 - top_k_categorical_accuracy: 0.8277\n",
      "Epoch 150/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1.7560 - accuracy: 0.6133 - categorical_accuracy: 0.6133 - top_k_categorical_accuracy: 0.8434\n",
      "Epoch 151/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1.5741 - accuracy: 0.6542 - categorical_accuracy: 0.6542 - top_k_categorical_accuracy: 0.8518\n",
      "Epoch 152/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1.5850 - accuracy: 0.6349 - categorical_accuracy: 0.6349 - top_k_categorical_accuracy: 0.8675\n",
      "Epoch 153/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1.8601 - accuracy: 0.6048 - categorical_accuracy: 0.6048 - top_k_categorical_accuracy: 0.8398\n",
      "Epoch 154/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1.7167 - accuracy: 0.6301 - categorical_accuracy: 0.6301 - top_k_categorical_accuracy: 0.8434\n",
      "Epoch 155/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1.6713 - accuracy: 0.6434 - categorical_accuracy: 0.6434 - top_k_categorical_accuracy: 0.8337\n",
      "Epoch 156/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1.6014 - accuracy: 0.6446 - categorical_accuracy: 0.6446 - top_k_categorical_accuracy: 0.8578\n",
      "Epoch 157/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1.7515 - accuracy: 0.6398 - categorical_accuracy: 0.6398 - top_k_categorical_accuracy: 0.8410\n",
      "Epoch 158/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1.7845 - accuracy: 0.6193 - categorical_accuracy: 0.6193 - top_k_categorical_accuracy: 0.8325\n",
      "Epoch 159/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1.7604 - accuracy: 0.6446 - categorical_accuracy: 0.6446 - top_k_categorical_accuracy: 0.8410\n",
      "Epoch 160/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1.6887 - accuracy: 0.6157 - categorical_accuracy: 0.6157 - top_k_categorical_accuracy: 0.8289\n",
      "Epoch 161/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1.5669 - accuracy: 0.6518 - categorical_accuracy: 0.6518 - top_k_categorical_accuracy: 0.8639\n",
      "Epoch 162/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1.5907 - accuracy: 0.6301 - categorical_accuracy: 0.6301 - top_k_categorical_accuracy: 0.8458\n",
      "Epoch 163/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1.7047 - accuracy: 0.6289 - categorical_accuracy: 0.6289 - top_k_categorical_accuracy: 0.8337\n",
      "Epoch 164/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1.7002 - accuracy: 0.6446 - categorical_accuracy: 0.6446 - top_k_categorical_accuracy: 0.8386\n",
      "Epoch 165/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1.8986 - accuracy: 0.6217 - categorical_accuracy: 0.6217 - top_k_categorical_accuracy: 0.8470\n",
      "Epoch 166/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1.7091 - accuracy: 0.6337 - categorical_accuracy: 0.6337 - top_k_categorical_accuracy: 0.8458\n",
      "Epoch 167/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1.8491 - accuracy: 0.6133 - categorical_accuracy: 0.6133 - top_k_categorical_accuracy: 0.8482\n",
      "Epoch 168/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1.6693 - accuracy: 0.6145 - categorical_accuracy: 0.6145 - top_k_categorical_accuracy: 0.8506\n",
      "Epoch 169/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1.7270 - accuracy: 0.6337 - categorical_accuracy: 0.6337 - top_k_categorical_accuracy: 0.8361\n",
      "Epoch 170/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1.6552 - accuracy: 0.6301 - categorical_accuracy: 0.6301 - top_k_categorical_accuracy: 0.8337\n",
      "Epoch 171/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1.6790 - accuracy: 0.6277 - categorical_accuracy: 0.6277 - top_k_categorical_accuracy: 0.8482\n",
      "Epoch 172/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1.6133 - accuracy: 0.6398 - categorical_accuracy: 0.6398 - top_k_categorical_accuracy: 0.8458\n",
      "Epoch 173/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1.8627 - accuracy: 0.6133 - categorical_accuracy: 0.6133 - top_k_categorical_accuracy: 0.8133\n",
      "Epoch 174/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 2.0010 - accuracy: 0.6157 - categorical_accuracy: 0.6157 - top_k_categorical_accuracy: 0.8108\n",
      "Epoch 175/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1.8669 - accuracy: 0.6229 - categorical_accuracy: 0.6229 - top_k_categorical_accuracy: 0.8133\n",
      "Epoch 176/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1.6570 - accuracy: 0.6470 - categorical_accuracy: 0.6470 - top_k_categorical_accuracy: 0.8434\n",
      "Epoch 177/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1.7237 - accuracy: 0.6277 - categorical_accuracy: 0.6277 - top_k_categorical_accuracy: 0.8494\n",
      "Epoch 178/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1.7627 - accuracy: 0.6157 - categorical_accuracy: 0.6157 - top_k_categorical_accuracy: 0.8337\n",
      "Epoch 179/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1.8207 - accuracy: 0.6253 - categorical_accuracy: 0.6253 - top_k_categorical_accuracy: 0.8289\n",
      "Epoch 180/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1.9556 - accuracy: 0.5928 - categorical_accuracy: 0.5928 - top_k_categorical_accuracy: 0.8337\n",
      "Epoch 181/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 2.0763 - accuracy: 0.5952 - categorical_accuracy: 0.5952 - top_k_categorical_accuracy: 0.8157\n",
      "Epoch 182/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1.9443 - accuracy: 0.6036 - categorical_accuracy: 0.6036 - top_k_categorical_accuracy: 0.8169\n",
      "Epoch 183/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1.8149 - accuracy: 0.6060 - categorical_accuracy: 0.6060 - top_k_categorical_accuracy: 0.8133\n",
      "Epoch 184/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1.8486 - accuracy: 0.6181 - categorical_accuracy: 0.6181 - top_k_categorical_accuracy: 0.7988\n",
      "Epoch 185/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1.8529 - accuracy: 0.6133 - categorical_accuracy: 0.6133 - top_k_categorical_accuracy: 0.8120\n",
      "Epoch 186/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1.8601 - accuracy: 0.6241 - categorical_accuracy: 0.6241 - top_k_categorical_accuracy: 0.8108\n",
      "Epoch 187/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1.8223 - accuracy: 0.6145 - categorical_accuracy: 0.6145 - top_k_categorical_accuracy: 0.8157\n",
      "Epoch 188/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 2.5294 - accuracy: 0.5952 - categorical_accuracy: 0.5952 - top_k_categorical_accuracy: 0.8072\n",
      "Epoch 189/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 2.6224 - accuracy: 0.5470 - categorical_accuracy: 0.5470 - top_k_categorical_accuracy: 0.8024\n",
      "Epoch 190/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 2.1122 - accuracy: 0.5735 - categorical_accuracy: 0.5735 - top_k_categorical_accuracy: 0.7831\n",
      "Epoch 191/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 2.2573 - accuracy: 0.5651 - categorical_accuracy: 0.5651 - top_k_categorical_accuracy: 0.7916\n",
      "Epoch 192/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1.9811 - accuracy: 0.5795 - categorical_accuracy: 0.5795 - top_k_categorical_accuracy: 0.8012\n",
      "Epoch 193/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1.7266 - accuracy: 0.6337 - categorical_accuracy: 0.6337 - top_k_categorical_accuracy: 0.8434\n",
      "Epoch 194/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1.9869 - accuracy: 0.5952 - categorical_accuracy: 0.5952 - top_k_categorical_accuracy: 0.8060\n",
      "Epoch 195/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 2.1013 - accuracy: 0.5855 - categorical_accuracy: 0.5855 - top_k_categorical_accuracy: 0.7976\n",
      "Epoch 196/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 2.0008 - accuracy: 0.6145 - categorical_accuracy: 0.6145 - top_k_categorical_accuracy: 0.8313\n",
      "Epoch 197/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 2.0067 - accuracy: 0.5976 - categorical_accuracy: 0.5976 - top_k_categorical_accuracy: 0.8241\n",
      "Epoch 198/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 2.3363 - accuracy: 0.5651 - categorical_accuracy: 0.5651 - top_k_categorical_accuracy: 0.7976\n",
      "Epoch 199/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1.9607 - accuracy: 0.5988 - categorical_accuracy: 0.5988 - top_k_categorical_accuracy: 0.8169\n",
      "Epoch 200/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 2.4633 - accuracy: 0.5663 - categorical_accuracy: 0.5663 - top_k_categorical_accuracy: 0.7952\n",
      "Epoch 201/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 2.0841 - accuracy: 0.5831 - categorical_accuracy: 0.5831 - top_k_categorical_accuracy: 0.8012\n",
      "Epoch 202/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 2.0986 - accuracy: 0.5711 - categorical_accuracy: 0.5711 - top_k_categorical_accuracy: 0.7904\n",
      "Epoch 203/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 2.3089 - accuracy: 0.5711 - categorical_accuracy: 0.5711 - top_k_categorical_accuracy: 0.7819\n",
      "Epoch 204/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1.9554 - accuracy: 0.5952 - categorical_accuracy: 0.5952 - top_k_categorical_accuracy: 0.8000\n",
      "Epoch 205/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 2.0430 - accuracy: 0.5831 - categorical_accuracy: 0.5831 - top_k_categorical_accuracy: 0.7916\n",
      "Epoch 206/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1.8649 - accuracy: 0.5928 - categorical_accuracy: 0.5928 - top_k_categorical_accuracy: 0.8060\n",
      "Epoch 207/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1.7381 - accuracy: 0.6169 - categorical_accuracy: 0.6169 - top_k_categorical_accuracy: 0.8325\n",
      "Epoch 208/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1.9022 - accuracy: 0.5976 - categorical_accuracy: 0.5976 - top_k_categorical_accuracy: 0.8169\n",
      "Epoch 209/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1.7716 - accuracy: 0.6096 - categorical_accuracy: 0.6096 - top_k_categorical_accuracy: 0.8181\n",
      "Epoch 210/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1.9921 - accuracy: 0.6000 - categorical_accuracy: 0.6000 - top_k_categorical_accuracy: 0.8000\n",
      "Epoch 211/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1.8030 - accuracy: 0.5964 - categorical_accuracy: 0.5964 - top_k_categorical_accuracy: 0.8084\n",
      "Epoch 212/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 2.0273 - accuracy: 0.5699 - categorical_accuracy: 0.5699 - top_k_categorical_accuracy: 0.8012\n",
      "Epoch 213/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1.9033 - accuracy: 0.5964 - categorical_accuracy: 0.5964 - top_k_categorical_accuracy: 0.8108\n",
      "Epoch 214/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1.8941 - accuracy: 0.6120 - categorical_accuracy: 0.6120 - top_k_categorical_accuracy: 0.8313\n",
      "Epoch 215/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1.9833 - accuracy: 0.6060 - categorical_accuracy: 0.6060 - top_k_categorical_accuracy: 0.8265\n",
      "Epoch 216/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 2.1406 - accuracy: 0.5940 - categorical_accuracy: 0.5940 - top_k_categorical_accuracy: 0.7904\n",
      "Epoch 217/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1.8178 - accuracy: 0.6265 - categorical_accuracy: 0.6265 - top_k_categorical_accuracy: 0.8181\n",
      "Epoch 218/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1.7349 - accuracy: 0.6301 - categorical_accuracy: 0.6301 - top_k_categorical_accuracy: 0.8289\n",
      "Epoch 219/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1.9614 - accuracy: 0.5892 - categorical_accuracy: 0.5892 - top_k_categorical_accuracy: 0.8036\n",
      "Epoch 220/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 2.0430 - accuracy: 0.5988 - categorical_accuracy: 0.5988 - top_k_categorical_accuracy: 0.8048\n",
      "Epoch 221/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1.9657 - accuracy: 0.6012 - categorical_accuracy: 0.6012 - top_k_categorical_accuracy: 0.8289\n",
      "Epoch 222/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1.9010 - accuracy: 0.6012 - categorical_accuracy: 0.6012 - top_k_categorical_accuracy: 0.8337\n",
      "Epoch 223/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1.9730 - accuracy: 0.6205 - categorical_accuracy: 0.6205 - top_k_categorical_accuracy: 0.8193\n",
      "Epoch 224/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1.9039 - accuracy: 0.6060 - categorical_accuracy: 0.6060 - top_k_categorical_accuracy: 0.8012\n",
      "Epoch 225/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1.9144 - accuracy: 0.6241 - categorical_accuracy: 0.6241 - top_k_categorical_accuracy: 0.8229\n",
      "Epoch 226/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 2.1023 - accuracy: 0.5988 - categorical_accuracy: 0.5988 - top_k_categorical_accuracy: 0.8108\n",
      "Epoch 227/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 2.2692 - accuracy: 0.5904 - categorical_accuracy: 0.5904 - top_k_categorical_accuracy: 0.7880\n",
      "Epoch 228/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1.9598 - accuracy: 0.5940 - categorical_accuracy: 0.5940 - top_k_categorical_accuracy: 0.8072\n",
      "Epoch 229/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1.9866 - accuracy: 0.5711 - categorical_accuracy: 0.5711 - top_k_categorical_accuracy: 0.8072\n",
      "Epoch 230/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1.9196 - accuracy: 0.6072 - categorical_accuracy: 0.6072 - top_k_categorical_accuracy: 0.8120\n",
      "Epoch 231/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 2.0702 - accuracy: 0.5771 - categorical_accuracy: 0.5771 - top_k_categorical_accuracy: 0.7916\n",
      "Epoch 232/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1.9773 - accuracy: 0.6108 - categorical_accuracy: 0.6108 - top_k_categorical_accuracy: 0.7988\n",
      "Epoch 233/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 2.0069 - accuracy: 0.5976 - categorical_accuracy: 0.5976 - top_k_categorical_accuracy: 0.8145\n",
      "Epoch 234/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1.7228 - accuracy: 0.6133 - categorical_accuracy: 0.6133 - top_k_categorical_accuracy: 0.8120\n",
      "Epoch 235/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1.9301 - accuracy: 0.5771 - categorical_accuracy: 0.5771 - top_k_categorical_accuracy: 0.8036\n",
      "Epoch 236/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 2.1139 - accuracy: 0.5759 - categorical_accuracy: 0.5759 - top_k_categorical_accuracy: 0.8181\n",
      "Epoch 237/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1.8945 - accuracy: 0.6048 - categorical_accuracy: 0.6048 - top_k_categorical_accuracy: 0.8120\n",
      "Epoch 238/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1.8188 - accuracy: 0.6205 - categorical_accuracy: 0.6205 - top_k_categorical_accuracy: 0.8217\n",
      "Epoch 239/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1.8546 - accuracy: 0.6096 - categorical_accuracy: 0.6096 - top_k_categorical_accuracy: 0.8157\n",
      "Epoch 240/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1.8339 - accuracy: 0.6060 - categorical_accuracy: 0.6060 - top_k_categorical_accuracy: 0.8012\n",
      "Epoch 241/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1.7280 - accuracy: 0.6229 - categorical_accuracy: 0.6229 - top_k_categorical_accuracy: 0.8120\n",
      "Epoch 242/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1.7849 - accuracy: 0.6325 - categorical_accuracy: 0.6325 - top_k_categorical_accuracy: 0.8373\n",
      "Epoch 243/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1.6657 - accuracy: 0.6361 - categorical_accuracy: 0.6361 - top_k_categorical_accuracy: 0.8301\n",
      "Epoch 244/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1.7871 - accuracy: 0.6084 - categorical_accuracy: 0.6084 - top_k_categorical_accuracy: 0.8205\n",
      "Epoch 245/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1.6534 - accuracy: 0.6241 - categorical_accuracy: 0.6241 - top_k_categorical_accuracy: 0.8542\n",
      "Epoch 246/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1.8441 - accuracy: 0.6024 - categorical_accuracy: 0.6024 - top_k_categorical_accuracy: 0.8229\n",
      "Epoch 247/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 2.1214 - accuracy: 0.5976 - categorical_accuracy: 0.5976 - top_k_categorical_accuracy: 0.8205\n",
      "Epoch 248/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 2.0713 - accuracy: 0.5904 - categorical_accuracy: 0.5904 - top_k_categorical_accuracy: 0.8108\n",
      "Epoch 249/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1.8120 - accuracy: 0.6108 - categorical_accuracy: 0.6108 - top_k_categorical_accuracy: 0.8193\n",
      "Epoch 250/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1.9248 - accuracy: 0.6157 - categorical_accuracy: 0.6157 - top_k_categorical_accuracy: 0.8205\n",
      "Epoch 251/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1.9439 - accuracy: 0.6000 - categorical_accuracy: 0.6000 - top_k_categorical_accuracy: 0.8157\n",
      "Epoch 252/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1.9649 - accuracy: 0.6181 - categorical_accuracy: 0.6181 - top_k_categorical_accuracy: 0.8036\n",
      "Epoch 253/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1.7972 - accuracy: 0.6157 - categorical_accuracy: 0.6157 - top_k_categorical_accuracy: 0.8301\n",
      "Epoch 254/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 2.0562 - accuracy: 0.5916 - categorical_accuracy: 0.5916 - top_k_categorical_accuracy: 0.7904\n",
      "Epoch 255/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1.8726 - accuracy: 0.5855 - categorical_accuracy: 0.5855 - top_k_categorical_accuracy: 0.7904\n",
      "Epoch 256/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1.9760 - accuracy: 0.6024 - categorical_accuracy: 0.6024 - top_k_categorical_accuracy: 0.8048\n",
      "Epoch 257/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1.9798 - accuracy: 0.6096 - categorical_accuracy: 0.6096 - top_k_categorical_accuracy: 0.8325\n",
      "Epoch 258/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1.9947 - accuracy: 0.5928 - categorical_accuracy: 0.5928 - top_k_categorical_accuracy: 0.8193\n",
      "Epoch 259/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 2.0890 - accuracy: 0.5892 - categorical_accuracy: 0.5892 - top_k_categorical_accuracy: 0.8084\n",
      "Epoch 260/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 2.1271 - accuracy: 0.5819 - categorical_accuracy: 0.5819 - top_k_categorical_accuracy: 0.7880\n",
      "Epoch 261/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1.9588 - accuracy: 0.6157 - categorical_accuracy: 0.6157 - top_k_categorical_accuracy: 0.8205\n",
      "Epoch 262/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1.9417 - accuracy: 0.6012 - categorical_accuracy: 0.6012 - top_k_categorical_accuracy: 0.8036\n",
      "Epoch 263/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1.9172 - accuracy: 0.6120 - categorical_accuracy: 0.6120 - top_k_categorical_accuracy: 0.8000\n",
      "Epoch 264/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1.8922 - accuracy: 0.6060 - categorical_accuracy: 0.6060 - top_k_categorical_accuracy: 0.8229\n",
      "Epoch 265/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 2.0821 - accuracy: 0.5916 - categorical_accuracy: 0.5916 - top_k_categorical_accuracy: 0.7880\n",
      "Epoch 266/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1.9073 - accuracy: 0.6060 - categorical_accuracy: 0.6060 - top_k_categorical_accuracy: 0.8084\n",
      "Epoch 267/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 2.3419 - accuracy: 0.5614 - categorical_accuracy: 0.5614 - top_k_categorical_accuracy: 0.7675\n",
      "Epoch 268/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 2.0483 - accuracy: 0.5892 - categorical_accuracy: 0.5892 - top_k_categorical_accuracy: 0.7747\n",
      "Epoch 269/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1.8035 - accuracy: 0.5964 - categorical_accuracy: 0.5964 - top_k_categorical_accuracy: 0.8133\n",
      "Epoch 270/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 2.0109 - accuracy: 0.5867 - categorical_accuracy: 0.5867 - top_k_categorical_accuracy: 0.8024\n",
      "Epoch 271/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 2.0035 - accuracy: 0.5904 - categorical_accuracy: 0.5904 - top_k_categorical_accuracy: 0.7880\n",
      "Epoch 272/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 2.0899 - accuracy: 0.5843 - categorical_accuracy: 0.5843 - top_k_categorical_accuracy: 0.8096\n",
      "Epoch 273/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1.8416 - accuracy: 0.6169 - categorical_accuracy: 0.6169 - top_k_categorical_accuracy: 0.8217\n",
      "Epoch 274/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1.9167 - accuracy: 0.5976 - categorical_accuracy: 0.5976 - top_k_categorical_accuracy: 0.7988\n",
      "Epoch 275/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1.8669 - accuracy: 0.6000 - categorical_accuracy: 0.6000 - top_k_categorical_accuracy: 0.8048\n",
      "Epoch 276/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 2.1214 - accuracy: 0.5566 - categorical_accuracy: 0.5566 - top_k_categorical_accuracy: 0.7807\n",
      "Epoch 277/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1.9296 - accuracy: 0.5988 - categorical_accuracy: 0.5988 - top_k_categorical_accuracy: 0.8024\n",
      "Epoch 278/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 2.0382 - accuracy: 0.5892 - categorical_accuracy: 0.5892 - top_k_categorical_accuracy: 0.8024\n",
      "Epoch 279/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1.8822 - accuracy: 0.6120 - categorical_accuracy: 0.6120 - top_k_categorical_accuracy: 0.8193\n",
      "Epoch 280/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1.9137 - accuracy: 0.5940 - categorical_accuracy: 0.5940 - top_k_categorical_accuracy: 0.7976\n",
      "Epoch 281/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1.9729 - accuracy: 0.6060 - categorical_accuracy: 0.6060 - top_k_categorical_accuracy: 0.8169\n",
      "Epoch 282/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 2.0079 - accuracy: 0.5976 - categorical_accuracy: 0.5976 - top_k_categorical_accuracy: 0.8048\n",
      "Epoch 283/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 2.0533 - accuracy: 0.5843 - categorical_accuracy: 0.5843 - top_k_categorical_accuracy: 0.7904\n",
      "Epoch 284/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 2.2319 - accuracy: 0.5723 - categorical_accuracy: 0.5723 - top_k_categorical_accuracy: 0.7855\n",
      "Epoch 285/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 2.1222 - accuracy: 0.5759 - categorical_accuracy: 0.5759 - top_k_categorical_accuracy: 0.7747\n",
      "Epoch 286/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1.8904 - accuracy: 0.6024 - categorical_accuracy: 0.6024 - top_k_categorical_accuracy: 0.8084\n",
      "Epoch 287/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1.8364 - accuracy: 0.6024 - categorical_accuracy: 0.6024 - top_k_categorical_accuracy: 0.8193\n",
      "Epoch 288/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 2.0288 - accuracy: 0.5988 - categorical_accuracy: 0.5988 - top_k_categorical_accuracy: 0.8024\n",
      "Epoch 289/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1.9223 - accuracy: 0.5880 - categorical_accuracy: 0.5880 - top_k_categorical_accuracy: 0.7940\n",
      "Epoch 290/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1.8882 - accuracy: 0.6024 - categorical_accuracy: 0.6024 - top_k_categorical_accuracy: 0.8060\n",
      "Epoch 291/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1.9199 - accuracy: 0.6145 - categorical_accuracy: 0.6145 - top_k_categorical_accuracy: 0.8145\n",
      "Epoch 292/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1.9428 - accuracy: 0.6036 - categorical_accuracy: 0.6036 - top_k_categorical_accuracy: 0.8012\n",
      "Epoch 293/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1.8249 - accuracy: 0.6108 - categorical_accuracy: 0.6108 - top_k_categorical_accuracy: 0.8253\n",
      "Epoch 294/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 2.0460 - accuracy: 0.5916 - categorical_accuracy: 0.5916 - top_k_categorical_accuracy: 0.7988\n",
      "Epoch 295/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1.7891 - accuracy: 0.6265 - categorical_accuracy: 0.6265 - top_k_categorical_accuracy: 0.8253\n",
      "Epoch 296/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1.9690 - accuracy: 0.6024 - categorical_accuracy: 0.6024 - top_k_categorical_accuracy: 0.8036\n",
      "Epoch 297/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 2.1159 - accuracy: 0.5807 - categorical_accuracy: 0.5807 - top_k_categorical_accuracy: 0.7976\n",
      "Epoch 298/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 2.0788 - accuracy: 0.5843 - categorical_accuracy: 0.5843 - top_k_categorical_accuracy: 0.7867\n",
      "Epoch 299/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 2.0736 - accuracy: 0.5952 - categorical_accuracy: 0.5952 - top_k_categorical_accuracy: 0.8024\n",
      "Epoch 300/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1.8465 - accuracy: 0.6036 - categorical_accuracy: 0.6036 - top_k_categorical_accuracy: 0.8000\n",
      "Epoch 301/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 2.1377 - accuracy: 0.5783 - categorical_accuracy: 0.5783 - top_k_categorical_accuracy: 0.7867\n",
      "Epoch 302/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1.8792 - accuracy: 0.5928 - categorical_accuracy: 0.5928 - top_k_categorical_accuracy: 0.8084\n",
      "Epoch 303/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 2.0182 - accuracy: 0.6024 - categorical_accuracy: 0.6024 - top_k_categorical_accuracy: 0.7904\n",
      "Epoch 304/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1.8437 - accuracy: 0.5916 - categorical_accuracy: 0.5916 - top_k_categorical_accuracy: 0.8169\n",
      "Epoch 305/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 2.1572 - accuracy: 0.5747 - categorical_accuracy: 0.5747 - top_k_categorical_accuracy: 0.7880\n",
      "Epoch 306/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1.8364 - accuracy: 0.6313 - categorical_accuracy: 0.6313 - top_k_categorical_accuracy: 0.8169\n",
      "Epoch 307/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1.7931 - accuracy: 0.6157 - categorical_accuracy: 0.6157 - top_k_categorical_accuracy: 0.8157\n",
      "Epoch 308/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 2.1319 - accuracy: 0.5867 - categorical_accuracy: 0.5867 - top_k_categorical_accuracy: 0.7867\n",
      "Epoch 309/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1.9151 - accuracy: 0.6036 - categorical_accuracy: 0.6036 - top_k_categorical_accuracy: 0.8060\n",
      "Epoch 310/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 2.2062 - accuracy: 0.5807 - categorical_accuracy: 0.5807 - top_k_categorical_accuracy: 0.7928\n",
      "Epoch 311/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 2.1219 - accuracy: 0.5940 - categorical_accuracy: 0.5940 - top_k_categorical_accuracy: 0.8060\n",
      "Epoch 312/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1.9039 - accuracy: 0.5964 - categorical_accuracy: 0.5964 - top_k_categorical_accuracy: 0.8072\n",
      "Epoch 313/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 2.2698 - accuracy: 0.5855 - categorical_accuracy: 0.5855 - top_k_categorical_accuracy: 0.7855\n",
      "Epoch 314/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 2.2799 - accuracy: 0.5699 - categorical_accuracy: 0.5699 - top_k_categorical_accuracy: 0.7831\n",
      "Epoch 315/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1.9361 - accuracy: 0.6024 - categorical_accuracy: 0.6024 - top_k_categorical_accuracy: 0.7976\n",
      "Epoch 316/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1.9862 - accuracy: 0.5759 - categorical_accuracy: 0.5759 - top_k_categorical_accuracy: 0.8096\n",
      "Epoch 317/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1.9873 - accuracy: 0.5916 - categorical_accuracy: 0.5916 - top_k_categorical_accuracy: 0.8048\n",
      "Epoch 318/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1.9053 - accuracy: 0.6145 - categorical_accuracy: 0.6145 - top_k_categorical_accuracy: 0.8120\n",
      "Epoch 319/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1.9428 - accuracy: 0.6120 - categorical_accuracy: 0.6120 - top_k_categorical_accuracy: 0.7928\n",
      "Epoch 320/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 2.1091 - accuracy: 0.5904 - categorical_accuracy: 0.5904 - top_k_categorical_accuracy: 0.7831\n",
      "Epoch 321/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1.9227 - accuracy: 0.6000 - categorical_accuracy: 0.6000 - top_k_categorical_accuracy: 0.7976\n",
      "Epoch 322/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1.7636 - accuracy: 0.6108 - categorical_accuracy: 0.6108 - top_k_categorical_accuracy: 0.7988\n",
      "Epoch 323/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1.9036 - accuracy: 0.6036 - categorical_accuracy: 0.6036 - top_k_categorical_accuracy: 0.8060\n",
      "Epoch 324/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1.8877 - accuracy: 0.6060 - categorical_accuracy: 0.6060 - top_k_categorical_accuracy: 0.8145\n",
      "Epoch 325/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1.7366 - accuracy: 0.6301 - categorical_accuracy: 0.6301 - top_k_categorical_accuracy: 0.8145\n",
      "Epoch 326/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 2.2024 - accuracy: 0.5831 - categorical_accuracy: 0.5831 - top_k_categorical_accuracy: 0.7855\n",
      "Epoch 327/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1.9737 - accuracy: 0.6024 - categorical_accuracy: 0.6024 - top_k_categorical_accuracy: 0.8265\n",
      "Epoch 328/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1.9524 - accuracy: 0.5916 - categorical_accuracy: 0.5916 - top_k_categorical_accuracy: 0.7964\n",
      "Epoch 329/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1.9092 - accuracy: 0.6036 - categorical_accuracy: 0.6036 - top_k_categorical_accuracy: 0.7964\n",
      "Epoch 330/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1.9273 - accuracy: 0.5867 - categorical_accuracy: 0.5867 - top_k_categorical_accuracy: 0.8024\n",
      "Epoch 331/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 2.3285 - accuracy: 0.5916 - categorical_accuracy: 0.5916 - top_k_categorical_accuracy: 0.7590\n",
      "Epoch 332/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1.8791 - accuracy: 0.6193 - categorical_accuracy: 0.6193 - top_k_categorical_accuracy: 0.8229\n",
      "Epoch 333/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1.9179 - accuracy: 0.6253 - categorical_accuracy: 0.6253 - top_k_categorical_accuracy: 0.8060\n",
      "Epoch 334/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1.8829 - accuracy: 0.5952 - categorical_accuracy: 0.5952 - top_k_categorical_accuracy: 0.7904\n",
      "Epoch 335/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1.7946 - accuracy: 0.6133 - categorical_accuracy: 0.6133 - top_k_categorical_accuracy: 0.8241\n",
      "Epoch 336/400\n",
      "166/166 [==============================] - 1s 6ms/step - loss: 1.9881 - accuracy: 0.6145 - categorical_accuracy: 0.6145 - top_k_categorical_accuracy: 0.7916\n",
      "Epoch 337/400\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 1.8405 - accuracy: 0.6181 - categorical_accuracy: 0.6181 - top_k_categorical_accuracy: 0.8000\n",
      "Epoch 338/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1.9444 - accuracy: 0.6036 - categorical_accuracy: 0.6036 - top_k_categorical_accuracy: 0.8036\n",
      "Epoch 339/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 2.1787 - accuracy: 0.5952 - categorical_accuracy: 0.5952 - top_k_categorical_accuracy: 0.8048\n",
      "Epoch 340/400\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 2.1049 - accuracy: 0.5807 - categorical_accuracy: 0.5807 - top_k_categorical_accuracy: 0.7928\n",
      "Epoch 341/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 2.1220 - accuracy: 0.5723 - categorical_accuracy: 0.5723 - top_k_categorical_accuracy: 0.7675\n",
      "Epoch 342/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1.9040 - accuracy: 0.5892 - categorical_accuracy: 0.5892 - top_k_categorical_accuracy: 0.8000\n",
      "Epoch 343/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1.8472 - accuracy: 0.6096 - categorical_accuracy: 0.6096 - top_k_categorical_accuracy: 0.8169\n",
      "Epoch 344/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 2.0108 - accuracy: 0.6048 - categorical_accuracy: 0.6048 - top_k_categorical_accuracy: 0.8012\n",
      "Epoch 345/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 2.1568 - accuracy: 0.5783 - categorical_accuracy: 0.5783 - top_k_categorical_accuracy: 0.7759\n",
      "Epoch 346/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 2.0659 - accuracy: 0.5867 - categorical_accuracy: 0.5867 - top_k_categorical_accuracy: 0.7952\n",
      "Epoch 347/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 2.3275 - accuracy: 0.5699 - categorical_accuracy: 0.5699 - top_k_categorical_accuracy: 0.7867\n",
      "Epoch 348/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 2.2219 - accuracy: 0.5964 - categorical_accuracy: 0.5964 - top_k_categorical_accuracy: 0.7867\n",
      "Epoch 349/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 2.1349 - accuracy: 0.5831 - categorical_accuracy: 0.5831 - top_k_categorical_accuracy: 0.7952\n",
      "Epoch 350/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 2.2117 - accuracy: 0.5651 - categorical_accuracy: 0.5651 - top_k_categorical_accuracy: 0.7831\n",
      "Epoch 351/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 2.0158 - accuracy: 0.5843 - categorical_accuracy: 0.5843 - top_k_categorical_accuracy: 0.7940\n",
      "Epoch 352/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1.8919 - accuracy: 0.5952 - categorical_accuracy: 0.5952 - top_k_categorical_accuracy: 0.8060\n",
      "Epoch 353/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1.9891 - accuracy: 0.5795 - categorical_accuracy: 0.5795 - top_k_categorical_accuracy: 0.8084\n",
      "Epoch 354/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 2.1309 - accuracy: 0.5880 - categorical_accuracy: 0.5880 - top_k_categorical_accuracy: 0.8012\n",
      "Epoch 355/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 2.3131 - accuracy: 0.5663 - categorical_accuracy: 0.5663 - top_k_categorical_accuracy: 0.7639\n",
      "Epoch 356/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 2.3343 - accuracy: 0.5819 - categorical_accuracy: 0.5819 - top_k_categorical_accuracy: 0.7892\n",
      "Epoch 357/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 2.2902 - accuracy: 0.5458 - categorical_accuracy: 0.5458 - top_k_categorical_accuracy: 0.7639\n",
      "Epoch 358/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 2.5323 - accuracy: 0.5663 - categorical_accuracy: 0.5663 - top_k_categorical_accuracy: 0.7699\n",
      "Epoch 359/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 2.0685 - accuracy: 0.5711 - categorical_accuracy: 0.5711 - top_k_categorical_accuracy: 0.7699\n",
      "Epoch 360/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 2.0955 - accuracy: 0.5807 - categorical_accuracy: 0.5807 - top_k_categorical_accuracy: 0.7663\n",
      "Epoch 361/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 2.1427 - accuracy: 0.5687 - categorical_accuracy: 0.5687 - top_k_categorical_accuracy: 0.7687\n",
      "Epoch 362/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 2.0282 - accuracy: 0.5976 - categorical_accuracy: 0.5976 - top_k_categorical_accuracy: 0.7855\n",
      "Epoch 363/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1.7919 - accuracy: 0.6060 - categorical_accuracy: 0.6060 - top_k_categorical_accuracy: 0.8000\n",
      "Epoch 364/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1.9938 - accuracy: 0.5843 - categorical_accuracy: 0.5843 - top_k_categorical_accuracy: 0.7964\n",
      "Epoch 365/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 2.0452 - accuracy: 0.5904 - categorical_accuracy: 0.5904 - top_k_categorical_accuracy: 0.7928\n",
      "Epoch 366/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 2.0372 - accuracy: 0.5964 - categorical_accuracy: 0.5964 - top_k_categorical_accuracy: 0.7952\n",
      "Epoch 367/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 2.1894 - accuracy: 0.5904 - categorical_accuracy: 0.5904 - top_k_categorical_accuracy: 0.7819\n",
      "Epoch 368/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1.9706 - accuracy: 0.5795 - categorical_accuracy: 0.5795 - top_k_categorical_accuracy: 0.7747\n",
      "Epoch 369/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 2.2022 - accuracy: 0.5723 - categorical_accuracy: 0.5723 - top_k_categorical_accuracy: 0.7627\n",
      "Epoch 370/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 2.1448 - accuracy: 0.5795 - categorical_accuracy: 0.5795 - top_k_categorical_accuracy: 0.7735\n",
      "Epoch 371/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1.9822 - accuracy: 0.6036 - categorical_accuracy: 0.6036 - top_k_categorical_accuracy: 0.8108\n",
      "Epoch 372/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 2.0865 - accuracy: 0.5904 - categorical_accuracy: 0.5904 - top_k_categorical_accuracy: 0.7964\n",
      "Epoch 373/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 2.0026 - accuracy: 0.5952 - categorical_accuracy: 0.5952 - top_k_categorical_accuracy: 0.7904\n",
      "Epoch 374/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1.8481 - accuracy: 0.6108 - categorical_accuracy: 0.6108 - top_k_categorical_accuracy: 0.8072\n",
      "Epoch 375/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 2.0403 - accuracy: 0.5807 - categorical_accuracy: 0.5807 - top_k_categorical_accuracy: 0.7831\n",
      "Epoch 376/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1.8533 - accuracy: 0.6096 - categorical_accuracy: 0.6096 - top_k_categorical_accuracy: 0.7964\n",
      "Epoch 377/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 2.0158 - accuracy: 0.6060 - categorical_accuracy: 0.6060 - top_k_categorical_accuracy: 0.8096\n",
      "Epoch 378/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1.9487 - accuracy: 0.5892 - categorical_accuracy: 0.5892 - top_k_categorical_accuracy: 0.7988\n",
      "Epoch 379/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1.9586 - accuracy: 0.6012 - categorical_accuracy: 0.6012 - top_k_categorical_accuracy: 0.7783\n",
      "Epoch 380/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1.8452 - accuracy: 0.5940 - categorical_accuracy: 0.5940 - top_k_categorical_accuracy: 0.8012\n",
      "Epoch 381/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1.9322 - accuracy: 0.5867 - categorical_accuracy: 0.5867 - top_k_categorical_accuracy: 0.7988\n",
      "Epoch 382/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 2.0536 - accuracy: 0.5831 - categorical_accuracy: 0.5831 - top_k_categorical_accuracy: 0.7699\n",
      "Epoch 383/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 2.1030 - accuracy: 0.5783 - categorical_accuracy: 0.5783 - top_k_categorical_accuracy: 0.7964\n",
      "Epoch 384/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1.9432 - accuracy: 0.5795 - categorical_accuracy: 0.5795 - top_k_categorical_accuracy: 0.7735\n",
      "Epoch 385/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1.8915 - accuracy: 0.5831 - categorical_accuracy: 0.5831 - top_k_categorical_accuracy: 0.7759\n",
      "Epoch 386/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1.9127 - accuracy: 0.5952 - categorical_accuracy: 0.5952 - top_k_categorical_accuracy: 0.8060\n",
      "Epoch 387/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1.8208 - accuracy: 0.5843 - categorical_accuracy: 0.5843 - top_k_categorical_accuracy: 0.8157\n",
      "Epoch 388/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 2.1746 - accuracy: 0.5904 - categorical_accuracy: 0.5904 - top_k_categorical_accuracy: 0.7843\n",
      "Epoch 389/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 2.3845 - accuracy: 0.5916 - categorical_accuracy: 0.5916 - top_k_categorical_accuracy: 0.7952\n",
      "Epoch 390/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 2.1874 - accuracy: 0.5940 - categorical_accuracy: 0.5940 - top_k_categorical_accuracy: 0.8048\n",
      "Epoch 391/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 2.3282 - accuracy: 0.5711 - categorical_accuracy: 0.5711 - top_k_categorical_accuracy: 0.7699\n",
      "Epoch 392/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1.9275 - accuracy: 0.5892 - categorical_accuracy: 0.5892 - top_k_categorical_accuracy: 0.7964\n",
      "Epoch 393/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 2.4608 - accuracy: 0.5446 - categorical_accuracy: 0.5446 - top_k_categorical_accuracy: 0.7627\n",
      "Epoch 394/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 2.2453 - accuracy: 0.5759 - categorical_accuracy: 0.5759 - top_k_categorical_accuracy: 0.7711\n",
      "Epoch 395/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 2.3695 - accuracy: 0.5614 - categorical_accuracy: 0.5614 - top_k_categorical_accuracy: 0.7651\n",
      "Epoch 396/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 2.2128 - accuracy: 0.5542 - categorical_accuracy: 0.5542 - top_k_categorical_accuracy: 0.7687\n",
      "Epoch 397/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 2.3474 - accuracy: 0.5855 - categorical_accuracy: 0.5855 - top_k_categorical_accuracy: 0.7855\n",
      "Epoch 398/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 2.0549 - accuracy: 0.5795 - categorical_accuracy: 0.5795 - top_k_categorical_accuracy: 0.7651\n",
      "Epoch 399/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1.9698 - accuracy: 0.5759 - categorical_accuracy: 0.5759 - top_k_categorical_accuracy: 0.7711\n",
      "Epoch 400/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 2.1802 - accuracy: 0.5843 - categorical_accuracy: 0.5843 - top_k_categorical_accuracy: 0.7783\n",
      "model created\n"
     ]
    }
   ],
   "source": [
    "#fitting and saving the model \n",
    "hist = model.fit(np.array(train_x), np.array(train_y), epochs=400, batch_size=5, verbose=1)\n",
    "model.save('model.h5', hist)\n",
    "print(\"model created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = model.evaluate(train_x[0:20], train_y[0:20], verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sklearn'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32me:\\Codes\\Major Project\\Mental-health-Chatbot\\IntentModel\\training\\training.ipynb Cell 10\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/e%3A/Codes/Major%20Project/Mental-health-Chatbot/IntentModel/training/training.ipynb#X13sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmetrics\u001b[39;00m \u001b[39mimport\u001b[39;00m confusion_matrix\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'sklearn'"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = hist.history['accuracy']\n",
    "val_acc = hist.history['categorical_accuracy']\n",
    "loss = hist.history['loss']\n",
    "val_loss = hist.history['top_k_categorical_accuracy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = {\n",
    "    \"acc\": acc,\n",
    "    \"val_acc\": val_acc,\n",
    "    \"loss\": loss,\n",
    "    \"top_k_categorical_accuracy\": val_loss\n",
    "}\n",
    "\n",
    "# Serializing json\n",
    "json_object = json.dumps(dictionary, indent=4)\n",
    " \n",
    "# Writing to sample.json\n",
    "with open(\"sample.json\", \"w\") as outfile:\n",
    "    outfile.write(json_object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'matplotlib'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32me:\\Codes\\Major Project\\Mental-health-Chatbot\\IntentModel\\training\\training.ipynb Cell 13\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/e%3A/Codes/Major%20Project/Mental-health-Chatbot/IntentModel/training/training.ipynb#X14sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mmatplotlib\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpyplot\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mplt\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/Codes/Major%20Project/Mental-health-Chatbot/IntentModel/training/training.ipynb#X14sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m epochs \u001b[39m=\u001b[39m \u001b[39mrange\u001b[39m(\u001b[39m1\u001b[39m, \u001b[39mlen\u001b[39m(acc) \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/Codes/Major%20Project/Mental-health-Chatbot/IntentModel/training/training.ipynb#X14sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m plt\u001b[39m.\u001b[39mplot(epochs, acc, \u001b[39m'\u001b[39m\u001b[39mbo\u001b[39m\u001b[39m'\u001b[39m, label\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mTraining acc\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'matplotlib'"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "epochs = range(1, len(acc) + 1)\n",
    "\n",
    "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "# plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "# plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "# plt.title('Training and validation loss')\n",
    "# plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c5c2a1cb3f7f6160aa571082c3edf6897403ba248133963500dc500635300624"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
